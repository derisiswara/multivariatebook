[["index.html", "Quantitative Methods with RStudio: Application for Management and Business Research Welcome", " Quantitative Methods with RStudio: Application for Management and Business Research Deri Siswara and Nafisa Berliana Indah Pratiwi Welcome This is the code version of Quantitative Methods with RStudio: Application for Management and Business Research, a book released in 2024 by IPB Press. The book was written by Muhammad Firdaus, Farit M Afendi, Deri Siswara, and Nafisa Berliana Indah Pratiwi. You can order the full version here, which includes more detailed explanations. Management quantitative analysis is widely utilized by students, lecturers, and researchers in Indonesia. This book aims to enhance the reputation of education and research in the country by presenting a variety of alternative analysis tools that are commonly used. Managers must accurately synthesize information during the decision-making process and prioritize various options precisely. Additionally, large volumes of transformed data such as customer identities and characteristics or consumer behavior survey results need to be synthesized properly. The first chapter introduces RStudio software and the R programming language, while the second chapter focuses on nonparametric statistical analysis including correlation analysis of two nonparametric variables and causality relationships. Chapter three discusses logistic regression analysis for making practical decisions, followed by discriminant analysis in chapter four which models problems involving one dependent variable influenced by multiple independent variables. Chapter five covers principal component analysis (PCA) and biplots to reduce a large selection of research variables into more compact dimensions. Chapter six delves into cluster analysis useful for mapping multiple entities whereas chapter seven comprehensively discusses factor analysis along with structural equation modeling (SEM), including PLS-SEM widely used for various problems involving latent variables such as prosperity, loyalty, and company performance. The final chapter explores Analytic Hierarchy Process (AHP) aimed at determining priority choices based on hierarchical decision hierarchy using freely accessible RStudio software across all methods presented in this book. Updates will be made frequently. This book may contain bugs/errors which readers can report at Buku.rstudio.ipb@gmail.com "],["basics-of-r.html", "Chapter 1 Basics of R 1.1 Introduction 1.2 Types of Objects in R 1.3 Data Frame Management 1.4 Visualization", " Chapter 1 Basics of R 1.1 Introduction A &lt;- 2 A # Print A #&gt; [1] 2 A = 2 A #&gt; [1] 2 B &lt;- &quot;Halo Semua&quot; B #&gt; [1] &quot;Halo Semua&quot; a&lt;-10 # Space is not sensitive but lettercase is sensitive. A #&gt; [1] 2 a #&gt; [1] 10 # Arithmetic operation x &lt;- 5 y &lt;- 3 x + y #&gt; [1] 8 x - y #&gt; [1] 2 x * y #&gt; [1] 15 x / y #&gt; [1] 1.666667 # Logic operation a &lt;- TRUE b &lt;- FALSE a &amp; b #&gt; [1] FALSE a | b #&gt; [1] TRUE !a #&gt; [1] FALSE x &lt;- 5 y &lt;- 3 x &gt; y #&gt; [1] TRUE x &lt; y #&gt; [1] FALSE x == y #&gt; [1] FALSE x &gt;= y #&gt; [1] TRUE x &lt;= y #&gt; [1] FALSE 1.2 Types of Objects in R 1.2.1 Vector a1 &lt;- c(2,4,7,3) # Numeric vector a2 &lt;- c(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;) # Character vector a3 &lt;- c(TRUE,TRUE,TRUE,FALSE,TRUE,FALSE) # Logical vector a1 #&gt; [1] 2 4 7 3 a3[4] #&gt; [1] FALSE a2[c(1,3)] #&gt; [1] &quot;one&quot; &quot;three&quot; a1[-1] #&gt; [1] 4 7 3 a1[2:4] #&gt; [1] 4 7 3 a &lt;- c(1, 2, 3) b &lt;- c(4, 5, 6) c &lt;- c(a, b) c #&gt; [1] 1 2 3 4 5 6 c[1:3] #&gt; [1] 1 2 3 d &lt;- a + b d #&gt; [1] 5 7 9 a4 &lt;- 1:12 b1 &lt;- matrix(a4,3,4) b2 &lt;- matrix(a4,3,4,byrow=TRUE) b3 &lt;- matrix(1:14,4,4) #&gt; Warning in matrix(1:14, 4, 4): data length [14] is not a #&gt; sub-multiple or multiple of the number of rows [4] b1 #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 4 7 10 #&gt; [2,] 2 5 8 11 #&gt; [3,] 3 6 9 12 b2 #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 2 3 4 #&gt; [2,] 5 6 7 8 #&gt; [3,] 9 10 11 12 b3 #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 5 9 13 #&gt; [2,] 2 6 10 14 #&gt; [3,] 3 7 11 1 #&gt; [4,] 4 8 12 2 b2[2,3] #&gt; [1] 7 b2[1:2,] #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 2 3 4 #&gt; [2,] 5 6 7 8 b2[c(1,3),-2] #&gt; [,1] [,2] [,3] #&gt; [1,] 1 3 4 #&gt; [2,] 9 11 12 dim(b2) #&gt; [1] 3 4 m1 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3) m2 &lt;- matrix(c(7, 8, 9, 10, 11, 12), nrow = 2, ncol = 3) m3 &lt;- m1 + m2 m3 #&gt; [,1] [,2] [,3] #&gt; [1,] 8 12 16 #&gt; [2,] 10 14 18 m4 &lt;- m1 %*% t(m2) m4 #&gt; [,1] [,2] #&gt; [1,] 89 98 #&gt; [2,] 116 128 1.2.2 Factor a5 &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;AB&quot;,&quot;O&quot;) d1 &lt;- factor(a5) levels(d1) #&gt; [1] &quot;A&quot; &quot;AB&quot; &quot;B&quot; &quot;O&quot; levels(d1) &lt;- c(&quot;Darah A&quot;,&quot;Darah AB&quot;,&quot;Darah B&quot;,&quot;Darah O&quot;) d1 #&gt; [1] Darah A Darah B Darah AB Darah O #&gt; Levels: Darah A Darah AB Darah B Darah O a6 &lt;- c(&quot;SMA&quot;,&quot;SD&quot;,&quot;SMP&quot;,&quot;SMA&quot;,&quot;SMA&quot;,&quot;SMA&quot;,&quot;SMA&quot;,&quot;SMA&quot;,&quot;SMA&quot;,&quot;SMA&quot;,&quot;SMA&quot;,&quot;SMA&quot;,&quot;SMA&quot;) d5 &lt;- factor(a6, levels=c(&quot;SD&quot;,&quot;SMP&quot;,&quot;SMA&quot;)) # Skala pengukuran ordinal levels(d5) #&gt; [1] &quot;SD&quot; &quot;SMP&quot; &quot;SMA&quot; d5 #&gt; [1] SMA SD SMP SMA SMA SMA SMA SMA SMA SMA SMA SMA SMA #&gt; Levels: SD SMP SMA 1.2.3 List a1; b2; d1 #&gt; [1] 2 4 7 3 #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 2 3 4 #&gt; [2,] 5 6 7 8 #&gt; [3,] 9 10 11 12 #&gt; [1] Darah A Darah B Darah AB Darah O #&gt; Levels: Darah A Darah AB Darah B Darah O e1 &lt;- list(a1,b2,d1) e2 &lt;- list(vect=a1,mat=b2,fac=d1) e1 #&gt; [[1]] #&gt; [1] 2 4 7 3 #&gt; #&gt; [[2]] #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 2 3 4 #&gt; [2,] 5 6 7 8 #&gt; [3,] 9 10 11 12 #&gt; #&gt; [[3]] #&gt; [1] Darah A Darah B Darah AB Darah O #&gt; Levels: Darah A Darah AB Darah B Darah O e2 #&gt; $vect #&gt; [1] 2 4 7 3 #&gt; #&gt; $mat #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 2 3 4 #&gt; [2,] 5 6 7 8 #&gt; [3,] 9 10 11 12 #&gt; #&gt; $fac #&gt; [1] Darah A Darah B Darah AB Darah O #&gt; Levels: Darah A Darah AB Darah B Darah O e1[[1]][2] #&gt; [1] 4 e2$fac #&gt; [1] Darah A Darah B Darah AB Darah O #&gt; Levels: Darah A Darah AB Darah B Darah O e2[2] #&gt; $mat #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 2 3 4 #&gt; [2,] 5 6 7 8 #&gt; [3,] 9 10 11 12 names(e2) #&gt; [1] &quot;vect&quot; &quot;mat&quot; &quot;fac&quot; 1.2.4 Data Frame Angka &lt;- 11:15 Huruf &lt;- factor(LETTERS[6:10]) f1 &lt;- data.frame(Angka,Huruf) f1 #&gt; Angka Huruf #&gt; 1 11 F #&gt; 2 12 G #&gt; 3 13 H #&gt; 4 14 I #&gt; 5 15 J f1[1,2] #&gt; [1] F #&gt; Levels: F G H I J f1$Angka #&gt; [1] 11 12 13 14 15 f1[,&quot;Huruf&quot;] #&gt; [1] F G H I J #&gt; Levels: F G H I J colnames(f1) #&gt; [1] &quot;Angka&quot; &quot;Huruf&quot; str(f1) #&gt; &#39;data.frame&#39;: 5 obs. of 2 variables: #&gt; $ Angka: int 11 12 13 14 15 #&gt; $ Huruf: Factor w/ 5 levels &quot;F&quot;,&quot;G&quot;,&quot;H&quot;,&quot;I&quot;,..: 1 2 3 4 5 1.3 Data Frame Management data(iris) head(iris) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa tail(iris) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 145 6.7 3.3 5.7 2.5 #&gt; 146 6.7 3.0 5.2 2.3 #&gt; 147 6.3 2.5 5.0 1.9 #&gt; 148 6.5 3.0 5.2 2.0 #&gt; 149 6.2 3.4 5.4 2.3 #&gt; 150 5.9 3.0 5.1 1.8 #&gt; Species #&gt; 145 virginica #&gt; 146 virginica #&gt; 147 virginica #&gt; 148 virginica #&gt; 149 virginica #&gt; 150 virginica str(iris) #&gt; &#39;data.frame&#39;: 150 obs. of 5 variables: #&gt; $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... #&gt; $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... #&gt; $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... #&gt; $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... #&gt; $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 1.3.1 R Package # install.packages(&quot;readxl&quot;) - code to install R package library(readxl) #install.packages(&quot;dplyr&quot;) library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union 1.3.2 Data Management With dplyr head(iris) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa irisbaru &lt;- mutate(iris, sepal2 = Sepal.Length + Sepal.Width) head(irisbaru) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; sepal2 #&gt; 1 8.6 #&gt; 2 7.9 #&gt; 3 7.9 #&gt; 4 7.7 #&gt; 5 8.6 #&gt; 6 9.3 irisetosa &lt;- filter(iris, Species==&quot;setosa&quot;) head(irisetosa) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa levels(iris$Species) #&gt; [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; irisversicolor &lt;- filter(iris, Species==&quot;setosa&quot;&amp; Petal.Length==1.3) head(irisversicolor) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 4.7 3.2 1.3 0.2 setosa #&gt; 2 5.4 3.9 1.3 0.4 setosa #&gt; 3 5.5 3.5 1.3 0.2 setosa #&gt; 4 4.4 3.0 1.3 0.2 setosa #&gt; 5 5.0 3.5 1.3 0.3 setosa #&gt; 6 4.5 2.3 1.3 0.3 setosa iris3 &lt;- select(iris, Sepal.Length, Species) head(iris3) #&gt; Sepal.Length Species #&gt; 1 5.1 setosa #&gt; 2 4.9 setosa #&gt; 3 4.7 setosa #&gt; 4 4.6 setosa #&gt; 5 5.0 setosa #&gt; 6 5.4 setosa iris4 &lt;- arrange(iris, Petal.Width) head(iris4) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 4.9 3.1 1.5 0.1 setosa #&gt; 2 4.8 3.0 1.4 0.1 setosa #&gt; 3 4.3 3.0 1.1 0.1 setosa #&gt; 4 5.2 4.1 1.5 0.1 setosa #&gt; 5 4.9 3.6 1.4 0.1 setosa #&gt; 6 5.1 3.5 1.4 0.2 setosa iris4 &lt;- arrange(iris, Species, desc(Petal.Width)) head(iris4) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.0 3.5 1.6 0.6 setosa #&gt; 2 5.1 3.3 1.7 0.5 setosa #&gt; 3 5.4 3.9 1.7 0.4 setosa #&gt; 4 5.7 4.4 1.5 0.4 setosa #&gt; 5 5.4 3.9 1.3 0.4 setosa #&gt; 6 5.1 3.7 1.5 0.4 setosa names(iris4)[1] &lt;- &quot;length&quot; head(iris4) #&gt; length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.0 3.5 1.6 0.6 setosa #&gt; 2 5.1 3.3 1.7 0.5 setosa #&gt; 3 5.4 3.9 1.7 0.4 setosa #&gt; 4 5.7 4.4 1.5 0.4 setosa #&gt; 5 5.4 3.9 1.3 0.4 setosa #&gt; 6 5.1 3.7 1.5 0.4 setosa head(iris4[,c(-1,-3)]) #&gt; Sepal.Width Petal.Width Species #&gt; 1 3.5 0.6 setosa #&gt; 2 3.3 0.5 setosa #&gt; 3 3.9 0.4 setosa #&gt; 4 4.4 0.4 setosa #&gt; 5 3.9 0.4 setosa #&gt; 6 3.7 0.4 setosa iris %&gt;% group_by(Species) %&gt;% summarise(rata2_Sepal.Width = mean(Sepal.Width)) #&gt; # A tibble: 3 × 2 #&gt; Species rata2_Sepal.Width #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 setosa 3.43 #&gt; 2 versicolor 2.77 #&gt; 3 virginica 2.97 1.4 Visualization 1.4.1 Histogram hist(iris$Sepal.Length) 1.4.2 Box Plot boxplot(iris$Sepal.Length) 1.4.3 Barplot table(iris$Species) #&gt; #&gt; setosa versicolor virginica #&gt; 50 50 50 barplot(table(iris$Species)) 1.4.4 Pie Chart pie(table(iris$Species)) 1.4.5 Scatter Plot plot(iris$Sepal.Length,iris$Sepal.Width) plot(iris$Sepal.Length, iris$Sepal.Width, main = &quot;Sepal Length vs. Sepal Width&quot;, xlab = &quot;Sepal Length&quot;, ylab = &quot;Sepal Width&quot;, col = &quot;red&quot;) "],["nonparametric-statistics.html", "Chapter 2 Nonparametric Statistics 2.1 Correlation 2.2 Difference Test", " Chapter 2 Nonparametric Statistics 2.1 Correlation # Membuat data contoh # Membuat vektor untuk responden, X, dan Y X &lt;- c(2, 1, 6, 11, 7, 11, 1, 12, 13, 13, 11) Y &lt;- c(9, 8, 16, 13, 11, 12, 7, 7, 13, 17, 10) # Membuat dataframe dataku &lt;- data.frame(X = X, Y = Y) # Menampilkan data dataku #&gt; X Y #&gt; 1 2 9 #&gt; 2 1 8 #&gt; 3 6 16 #&gt; 4 11 13 #&gt; 5 7 11 #&gt; 6 11 12 #&gt; 7 1 7 #&gt; 8 12 7 #&gt; 9 13 13 #&gt; 10 13 17 #&gt; 11 11 10 # Menggunakan fungsi cor.test untuk menghitung Tau-Kendall cor.test(dataku$X, dataku$Y, method = &quot;kendall&quot;) #&gt; Warning in cor.test.default(dataku$X, dataku$Y, method = #&gt; &quot;kendall&quot;): Cannot compute exact p-value with ties #&gt; #&gt; Kendall&#39;s rank correlation tau #&gt; #&gt; data: dataku$X and dataku$Y #&gt; z = 1.7529, p-value = 0.07962 #&gt; alternative hypothesis: true tau is not equal to 0 #&gt; sample estimates: #&gt; tau #&gt; 0.4273658 # Menggunakan fungsi cor.test untuk menghitung korelasi Spearman cor.test(dataku$X, dataku$Y, method = &quot;spearman&quot;) #&gt; Warning in cor.test.default(dataku$X, dataku$Y, method = #&gt; &quot;spearman&quot;): Cannot compute exact p-value with ties #&gt; #&gt; Spearman&#39;s rank correlation rho #&gt; #&gt; data: dataku$X and dataku$Y #&gt; S = 108.98, p-value = 0.1134 #&gt; alternative hypothesis: true rho is not equal to 0 #&gt; sample estimates: #&gt; rho #&gt; 0.5046513 2.1.1 Chi-Square Test # Membuat data contoh # Data asli dalam bentuk tabel silang frekuensi &lt;- matrix(c(30, 21, 30, 19, 15, 35), nrow = 2) rownames(frekuensi) &lt;- c(&quot;Kontrak&quot;, &quot;Tetap&quot;) colnames(frekuensi) &lt;- c(&quot;Rendah&quot;, &quot;Sedang&quot;, &quot;Tinggi&quot;) # Inisiasi vektor kosong untuk menyimpan data Status_Pegawai &lt;- c() Tingkat_Produktivitas &lt;- c() # Mengulang setiap kombinasi sesuai dengan frekuensinya for (i in 1:nrow(frekuensi)) { for (j in 1:ncol(frekuensi)) { Status_Pegawai &lt;- c(Status_Pegawai, rep(rownames(frekuensi)[i], frekuensi[i, j])) Tingkat_Produktivitas &lt;- c(Tingkat_Produktivitas, rep(colnames(frekuensi)[j], frekuensi[i, j])) } } # Membuat dataframe dataku2 &lt;- data.frame(Status_Pegawai, Tingkat_Produktivitas) # Menampilkan data head(dataku2) #&gt; Status_Pegawai Tingkat_Produktivitas #&gt; 1 Kontrak Rendah #&gt; 2 Kontrak Rendah #&gt; 3 Kontrak Rendah #&gt; 4 Kontrak Rendah #&gt; 5 Kontrak Rendah #&gt; 6 Kontrak Rendah # Transformasi menejadi factor dataku2$Status_Pegawai &lt;- as.factor(dataku2$Status_Pegawai) dataku2$Tingkat_Produktivitas &lt;- as.factor(dataku2$Tingkat_Produktivitas) summary(dataku2) #&gt; Status_Pegawai Tingkat_Produktivitas #&gt; Kontrak:75 Rendah:51 #&gt; Tetap :75 Sedang:49 #&gt; Tinggi:50 # Melakukan tabel kontingensi dataku2_kt &lt;- table(dataku2$Status_Pegawai, dataku2$Tingkat_Produktivitas) dataku2_kt #&gt; #&gt; Rendah Sedang Tinggi #&gt; Kontrak 30 30 15 #&gt; Tetap 21 19 35 # Melakukan uji Chi-Square chisq.test(dataku2_kt) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: dataku2_kt #&gt; X-squared = 12.058, df = 2, p-value = 0.002408 2.2 Difference Test 2.2.1 Two sample test (Independent) 2.2.1.1 Mann-Whitney Test # Membuat data contoh # Vektor data untuk efisiensi pada skala besar dan kecil efisiensi_besar &lt;- c(1.31, 1.25, 1.32, 1.3, 1.33, 1.31, 1.35, 1.34, 0.28, 1.34, 1.28) efisiensi_kecil &lt;- c(1.21, 1.28, 1.32, 1.25, 1.27, 1.31, 1.26, 1.31, 1.24, 1.22) wilcox.test(efisiensi_besar, efisiensi_kecil) #&gt; Warning in wilcox.test.default(efisiensi_besar, #&gt; efisiensi_kecil): cannot compute exact p-value with ties #&gt; #&gt; Wilcoxon rank sum test with continuity correction #&gt; #&gt; data: efisiensi_besar and efisiensi_kecil #&gt; W = 82.5, p-value = 0.05614 #&gt; alternative hypothesis: true location shift is not equal to 0 2.2.1.2 Chi-Square Test 2.2.2 More than two sample test (Independent) 2.2.2.1 Kruskal-Wallis Test # Membuat data contoh # Membuat vektor untuk Industri A, B, dan C industri_A &lt;- c(2.33, 2.79, 3.01, 2.33, 1.22, 2.79, 1.9, 1.65) industri_B &lt;- c(2.33, 2.33, 2.79, 3.01, 1.99, 2.45) industri_C &lt;- c(1.06, 1.37, 1.09, 1.65, 1.44, 1.11) # Membuat vektor industri industri &lt;- c(rep(&quot;Industri A&quot;, length(industri_A)), rep(&quot;Industri B&quot;, length(industri_B)), rep(&quot;Industri C&quot;, length(industri_C))) # Menggabungkan semua vektor value nilai &lt;- c(industri_A, industri_B, industri_C) # Membuat data frame dataku4 &lt;- data.frame(industri, nilai) # Menampilkan data frame dataku4$industri &lt;- as.factor(dataku4$industri) dataku4 #&gt; industri nilai #&gt; 1 Industri A 2.33 #&gt; 2 Industri A 2.79 #&gt; 3 Industri A 3.01 #&gt; 4 Industri A 2.33 #&gt; 5 Industri A 1.22 #&gt; 6 Industri A 2.79 #&gt; 7 Industri A 1.90 #&gt; 8 Industri A 1.65 #&gt; 9 Industri B 2.33 #&gt; 10 Industri B 2.33 #&gt; 11 Industri B 2.79 #&gt; 12 Industri B 3.01 #&gt; 13 Industri B 1.99 #&gt; 14 Industri B 2.45 #&gt; 15 Industri C 1.06 #&gt; 16 Industri C 1.37 #&gt; 17 Industri C 1.09 #&gt; 18 Industri C 1.65 #&gt; 19 Industri C 1.44 #&gt; 20 Industri C 1.11 # Uji kruskal wallis kruskal.test(nilai ~ industri, data = dataku4) #&gt; #&gt; Kruskal-Wallis rank sum test #&gt; #&gt; data: nilai by industri #&gt; Kruskal-Wallis chi-squared = 10.619, df = 2, p-value #&gt; = 0.004943 # Post hoc kruskal-wallis - Uji Dun #installed.packages(&quot;FSA&quot;) library(FSA) #&gt; Warning: package &#39;FSA&#39; was built under R version 4.4.3 #&gt; ## FSA v0.10.0. See citation(&#39;FSA&#39;) if used in publication. #&gt; ## Run fishR() for related website and fishR(&#39;IFAR&#39;) for related book. dunnTest(nilai ~ industri, data = dataku4) #&gt; Dunn (1964) Kruskal-Wallis multiple comparison #&gt; p-values adjusted with the Holm method. #&gt; Comparison Z P.unadj P.adj #&gt; 1 Industri A - Industri B -0.6428883 0.520296550 0.52029655 #&gt; 2 Industri A - Industri C 2.6109139 0.009030062 0.01806012 #&gt; 3 Industri B - Industri C 3.0436533 0.002337243 0.00701173 2.2.2.2 Chi-Square Test 2.2.3 Two sample test (Dependent) 2.2.3.1 Sign Test # Membuat data contoh # Data Skor Kepuasan produk_lama &lt;- c(16, 15, 18, 16, 17, 18, 20, 15, 14, 16, 19, 17) produk_baru &lt;- c(18, 17, 16, 19, 17, 20, 18, 16, 15, 18, 20, 18) # Data Responden responden &lt;- c(1:12) # Membuat data frame dataku5 &lt;- data.frame(Responden = c(rep(responden, 2)), Produk = factor(c(rep(&quot;Produk Lama&quot;, length(produk_lama)), rep(&quot;Produk Baru&quot;, length(produk_baru)))), Skor_Kepuasan = c(produk_lama, produk_baru)) # Menampilkan data frame dataku5 #&gt; Responden Produk Skor_Kepuasan #&gt; 1 1 Produk Lama 16 #&gt; 2 2 Produk Lama 15 #&gt; 3 3 Produk Lama 18 #&gt; 4 4 Produk Lama 16 #&gt; 5 5 Produk Lama 17 #&gt; 6 6 Produk Lama 18 #&gt; 7 7 Produk Lama 20 #&gt; 8 8 Produk Lama 15 #&gt; 9 9 Produk Lama 14 #&gt; 10 10 Produk Lama 16 #&gt; 11 11 Produk Lama 19 #&gt; 12 12 Produk Lama 17 #&gt; 13 1 Produk Baru 18 #&gt; 14 2 Produk Baru 17 #&gt; 15 3 Produk Baru 16 #&gt; 16 4 Produk Baru 19 #&gt; 17 5 Produk Baru 17 #&gt; 18 6 Produk Baru 20 #&gt; 19 7 Produk Baru 18 #&gt; 20 8 Produk Baru 16 #&gt; 21 9 Produk Baru 15 #&gt; 22 10 Produk Baru 18 #&gt; 23 11 Produk Baru 20 #&gt; 24 12 Produk Baru 18 # Menghitung perbedaan diff &lt;- dataku5[dataku5$Produk == &#39;Produk Baru&#39;, ]$Skor_Kepuasan - dataku5[dataku5$Produk == &#39;Produk Lama&#39;, ]$Skor_Kepuasan # Menghitung jumlah perbedaan yang positif jumlah_positif &lt;- sum(diff &gt; 0) # Melakukan uji tanda binom.test(jumlah_positif, length(diff), p = 0.5, alternative = &quot;two.sided&quot;) #&gt; #&gt; Exact binomial test #&gt; #&gt; data: jumlah_positif and length(diff) #&gt; number of successes = 9, number of trials = 12, #&gt; p-value = 0.146 #&gt; alternative hypothesis: true probability of success is not equal to 0.5 #&gt; 95 percent confidence interval: #&gt; 0.4281415 0.9451394 #&gt; sample estimates: #&gt; probability of success #&gt; 0.75 Interpretation: https://www.geeksforgeeks.org/sign-test-in-r/ 2.2.4 More than two sample test (Dependent) 2.2.4.1 Friedman Test # Membuat data contoh dataku6 &lt;- matrix(c(1.24,1.50,1.62, 1.71,1.85,2.05, 1.37,2.12,1.68, 2.53,1.87,2.62, 1.23,1.34,1.51, 1.94,2.33,2.86, 1.72,1.43,2.86), nrow = 7, byrow = TRUE, dimnames = list(Person= as.character(1:7), Obat = c(&quot;Obat A&quot;,&quot;Obat B&quot;,&quot;Obat C&quot;))) dataku6 #&gt; Obat #&gt; Person Obat A Obat B Obat C #&gt; 1 1.24 1.50 1.62 #&gt; 2 1.71 1.85 2.05 #&gt; 3 1.37 2.12 1.68 #&gt; 4 2.53 1.87 2.62 #&gt; 5 1.23 1.34 1.51 #&gt; 6 1.94 2.33 2.86 #&gt; 7 1.72 1.43 2.86 friedman.test(dataku6) #&gt; #&gt; Friedman rank sum test #&gt; #&gt; data: dataku6 #&gt; Friedman chi-squared = 8.8571, df = 2, p-value = #&gt; 0.01193 2.2.4.2 Cochran Test # Membuat data contoh ## Input data responden &lt;- c(1:8) produk_A &lt;- c(&quot;Tidak&quot;,&quot;Tidak&quot;,&quot;Ya&quot;,&quot;Ya&quot;,&quot;Ya&quot;,&quot;Tidak&quot;,&quot;Tidak&quot;,&quot;Tidak&quot;) produk_B &lt;- c(&quot;Tidak&quot;,&quot;Ya&quot;,&quot;Ya&quot;,&quot;Ya&quot;,&quot;Tidak&quot;,&quot;Tidak&quot;,&quot;Ya&quot;,&quot;Tidak&quot;) produk_C &lt;- c(&quot;Ya&quot;,&quot;Tidak&quot;,&quot;Tidak&quot;,&quot;Ya&quot;,&quot;Tidak&quot;,&quot;Ya&quot;,&quot;Ya&quot;,&quot;Tidak&quot;) dataku7 &lt;- data.frame(responden, produk_A, produk_B, produk_C) dataku7$produk_A &lt;- as.factor(dataku7$produk_A) dataku7$produk_B &lt;- as.factor(dataku7$produk_B) dataku7$produk_C &lt;- as.factor(dataku7$produk_C) dataku7 #&gt; responden produk_A produk_B produk_C #&gt; 1 1 Tidak Tidak Ya #&gt; 2 2 Tidak Ya Tidak #&gt; 3 3 Ya Ya Tidak #&gt; 4 4 Ya Ya Ya #&gt; 5 5 Ya Tidak Tidak #&gt; 6 6 Tidak Tidak Ya #&gt; 7 7 Tidak Ya Ya #&gt; 8 8 Tidak Tidak Tidak dataku7 &lt;-ifelse(dataku7==&quot;Ya&quot;, 1,0) #install.packages(&quot;nonpar&quot;) library(nonpar) cochrans.q(as.matrix(dataku7[,-1]), alpha = 0.05) #&gt; #&gt; Cochran&#39;s Q Test #&gt; #&gt; H0: There is no difference in the effectiveness of treatments. #&gt; HA: There is a difference in the effectiveness of treatments. #&gt; #&gt; Q = 0.333333333333333 #&gt; #&gt; Degrees of Freedom = 2 #&gt; #&gt; Significance Level = 0.05 #&gt; The p-value is 0.846481724890614 #&gt; #&gt; "],["logistic-regression.html", "Chapter 3 Logistic Regression 3.1 Regresi Logistik Biner 3.2 Regresi Logistik Nominal atau Multinominal 3.3 Regresi Logistik Ordinal", " Chapter 3 Logistic Regression 3.1 Regresi Logistik Biner 3.1.1 Data credit &lt;- read.csv(&quot;Data/credit.csv&quot;) head(credit[,1:5],10) #&gt; creditability account.balance duration credit.amount #&gt; 1 1 1 18 1049 #&gt; 2 1 1 9 2799 #&gt; 3 1 2 12 841 #&gt; 4 1 1 12 2122 #&gt; 5 1 1 12 2171 #&gt; 6 1 1 10 2241 #&gt; 7 1 1 8 3398 #&gt; 8 1 1 6 1361 #&gt; 9 1 4 18 1098 #&gt; 10 1 2 24 3758 #&gt; saving.balance #&gt; 1 1 #&gt; 2 1 #&gt; 3 2 #&gt; 4 1 #&gt; 5 1 #&gt; 6 1 #&gt; 7 1 #&gt; 8 1 #&gt; 9 1 #&gt; 10 3 str(credit) #&gt; &#39;data.frame&#39;: 1000 obs. of 14 variables: #&gt; $ creditability : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ account.balance : int 1 1 2 1 1 1 1 1 4 2 ... #&gt; $ duration : int 18 9 12 12 12 10 8 6 18 24 ... #&gt; $ credit.amount : int 1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ... #&gt; $ saving.balance : int 1 1 2 1 1 1 1 1 1 3 ... #&gt; $ employment.year : int 2 3 4 3 3 2 4 2 1 1 ... #&gt; $ installment.rate: int 4 2 2 3 4 1 1 2 4 1 ... #&gt; $ marital.status : int 2 3 2 3 3 3 3 3 2 2 ... #&gt; $ duration.address: int 4 2 4 2 4 3 4 4 4 4 ... #&gt; $ age : int 21 36 23 39 38 48 39 40 65 23 ... #&gt; $ dependents : int 1 2 1 2 1 2 1 2 1 1 ... #&gt; $ number.of.credit: int 1 2 1 2 2 2 2 1 2 1 ... #&gt; $ occupation : int 3 3 2 2 2 2 2 2 1 1 ... #&gt; $ previous.credit : int 4 4 2 4 4 4 4 4 4 2 ... library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union credit &lt;- credit %&gt;% mutate(across(-c(duration, credit.amount, age),as.factor)) str(credit) #&gt; &#39;data.frame&#39;: 1000 obs. of 14 variables: #&gt; $ creditability : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ account.balance : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 2 1 1 1 1 1 4 2 ... #&gt; $ duration : int 18 9 12 12 12 10 8 6 18 24 ... #&gt; $ credit.amount : int 1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ... #&gt; $ saving.balance : Factor w/ 5 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 2 1 1 1 1 1 1 3 ... #&gt; $ employment.year : Factor w/ 5 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 2 3 4 3 3 2 4 2 1 1 ... #&gt; $ installment.rate: Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 4 2 2 3 4 1 1 2 4 1 ... #&gt; $ marital.status : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 2 3 2 3 3 3 3 3 2 2 ... #&gt; $ duration.address: Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 4 2 4 2 4 3 4 4 4 4 ... #&gt; $ age : int 21 36 23 39 38 48 39 40 65 23 ... #&gt; $ dependents : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 2 1 2 1 2 1 2 1 1 ... #&gt; $ number.of.credit: Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 2 1 2 2 2 2 1 2 1 ... #&gt; $ occupation : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 3 3 2 2 2 2 2 2 1 1 ... #&gt; $ previous.credit : Factor w/ 5 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 5 5 3 5 5 5 5 5 5 3 ... 3.1.2 Pemodelan logreg1 &lt;- glm(creditability~.,data=credit,family = &quot;binomial&quot;) summary(logreg1) #&gt; #&gt; Call: #&gt; glm(formula = creditability ~ ., family = &quot;binomial&quot;, data = credit) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 2.990e-01 8.942e-01 0.334 0.738097 #&gt; account.balance2 4.346e-01 2.013e-01 2.159 0.030852 #&gt; account.balance3 9.490e-01 3.602e-01 2.635 0.008421 #&gt; account.balance4 1.804e+00 2.222e-01 8.119 4.69e-16 #&gt; duration -2.705e-02 8.818e-03 -3.068 0.002156 #&gt; credit.amount -1.025e-04 4.161e-05 -2.465 0.013718 #&gt; saving.balance2 1.293e-01 2.701e-01 0.479 0.632222 #&gt; saving.balance3 4.144e-01 3.987e-01 1.039 0.298644 #&gt; saving.balance4 1.241e+00 5.032e-01 2.467 0.013629 #&gt; saving.balance5 8.811e-01 2.463e-01 3.577 0.000347 #&gt; employment.year2 -1.432e-01 4.109e-01 -0.348 0.727561 #&gt; employment.year3 2.530e-01 3.957e-01 0.639 0.522582 #&gt; employment.year4 7.646e-01 4.258e-01 1.796 0.072572 #&gt; employment.year5 2.386e-01 3.962e-01 0.602 0.547012 #&gt; installment.rate2 -2.841e-01 2.953e-01 -0.962 0.336089 #&gt; installment.rate3 -5.122e-01 3.217e-01 -1.592 0.111374 #&gt; installment.rate4 -9.279e-01 2.872e-01 -3.230 0.001236 #&gt; marital.status2 1.744e-01 3.742e-01 0.466 0.641255 #&gt; marital.status3 7.482e-01 3.670e-01 2.039 0.041468 #&gt; marital.status4 5.577e-01 4.371e-01 1.276 0.201928 #&gt; duration.address2 -7.104e-01 2.832e-01 -2.509 0.012122 #&gt; duration.address3 -5.443e-01 3.163e-01 -1.721 0.085314 #&gt; duration.address4 -4.386e-01 2.762e-01 -1.588 0.112244 #&gt; age 1.125e-02 8.468e-03 1.329 0.183990 #&gt; dependents2 -2.607e-01 2.387e-01 -1.092 0.274669 #&gt; number.of.credit2 -4.177e-01 2.315e-01 -1.805 0.071133 #&gt; number.of.credit3 -4.131e-01 5.951e-01 -0.694 0.487625 #&gt; number.of.credit4 -4.589e-01 9.908e-01 -0.463 0.643240 #&gt; occupation2 -8.953e-02 6.276e-01 -0.143 0.886557 #&gt; occupation3 -1.487e-01 6.048e-01 -0.246 0.805804 #&gt; occupation4 1.276e-02 6.087e-01 0.021 0.983277 #&gt; previous.credit1 -3.136e-01 5.178e-01 -0.606 0.544686 #&gt; previous.credit2 6.063e-01 4.149e-01 1.461 0.143896 #&gt; previous.credit3 8.090e-01 4.531e-01 1.785 0.074205 #&gt; previous.credit4 1.511e+00 4.169e-01 3.625 0.000288 #&gt; #&gt; (Intercept) #&gt; account.balance2 * #&gt; account.balance3 ** #&gt; account.balance4 *** #&gt; duration ** #&gt; credit.amount * #&gt; saving.balance2 #&gt; saving.balance3 #&gt; saving.balance4 * #&gt; saving.balance5 *** #&gt; employment.year2 #&gt; employment.year3 #&gt; employment.year4 . #&gt; employment.year5 #&gt; installment.rate2 #&gt; installment.rate3 #&gt; installment.rate4 ** #&gt; marital.status2 #&gt; marital.status3 * #&gt; marital.status4 #&gt; duration.address2 * #&gt; duration.address3 . #&gt; duration.address4 #&gt; age #&gt; dependents2 #&gt; number.of.credit2 . #&gt; number.of.credit3 #&gt; number.of.credit4 #&gt; occupation2 #&gt; occupation3 #&gt; occupation4 #&gt; previous.credit1 #&gt; previous.credit2 #&gt; previous.credit3 . #&gt; previous.credit4 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 1221.7 on 999 degrees of freedom #&gt; Residual deviance: 956.0 on 965 degrees of freedom #&gt; AIC: 1026 #&gt; #&gt; Number of Fisher Scoring iterations: 5 3.1.3 Odds Ratio beta = round(coef(logreg1),2) OR = round(exp(beta),2) cbind(beta, OR) #&gt; beta OR #&gt; (Intercept) 0.30 1.35 #&gt; account.balance2 0.43 1.54 #&gt; account.balance3 0.95 2.59 #&gt; account.balance4 1.80 6.05 #&gt; duration -0.03 0.97 #&gt; credit.amount 0.00 1.00 #&gt; saving.balance2 0.13 1.14 #&gt; saving.balance3 0.41 1.51 #&gt; saving.balance4 1.24 3.46 #&gt; saving.balance5 0.88 2.41 #&gt; employment.year2 -0.14 0.87 #&gt; employment.year3 0.25 1.28 #&gt; employment.year4 0.76 2.14 #&gt; employment.year5 0.24 1.27 #&gt; installment.rate2 -0.28 0.76 #&gt; installment.rate3 -0.51 0.60 #&gt; installment.rate4 -0.93 0.39 #&gt; marital.status2 0.17 1.19 #&gt; marital.status3 0.75 2.12 #&gt; marital.status4 0.56 1.75 #&gt; duration.address2 -0.71 0.49 #&gt; duration.address3 -0.54 0.58 #&gt; duration.address4 -0.44 0.64 #&gt; age 0.01 1.01 #&gt; dependents2 -0.26 0.77 #&gt; number.of.credit2 -0.42 0.66 #&gt; number.of.credit3 -0.41 0.66 #&gt; number.of.credit4 -0.46 0.63 #&gt; occupation2 -0.09 0.91 #&gt; occupation3 -0.15 0.86 #&gt; occupation4 0.01 1.01 #&gt; previous.credit1 -0.31 0.73 #&gt; previous.credit2 0.61 1.84 #&gt; previous.credit3 0.81 2.25 #&gt; previous.credit4 1.51 4.53 3.1.4 Multikolineratitas library(car) #&gt; Loading required package: carData #&gt; #&gt; Attaching package: &#39;car&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; recode vif(logreg1) #&gt; GVIF Df GVIF^(1/(2*Df)) #&gt; account.balance 1.283532 3 1.042480 #&gt; duration 1.828834 1 1.352344 #&gt; credit.amount 2.284117 1 1.511330 #&gt; saving.balance 1.286469 4 1.031989 #&gt; employment.year 2.406179 4 1.116005 #&gt; installment.rate 1.443706 3 1.063114 #&gt; marital.status 1.439516 3 1.062599 #&gt; duration.address 1.502426 3 1.070201 #&gt; age 1.365556 1 1.168570 #&gt; dependents 1.177252 1 1.085012 #&gt; number.of.credit 2.060162 3 1.128020 #&gt; occupation 1.893863 3 1.112307 #&gt; previous.credit 2.136438 4 1.099541 3.1.5 Akurasi pred_clas &lt;- ifelse(logreg1$fitted.values &gt; 0.5, 1, 0) conf_matrix &lt;- table(credit$creditability, pred_clas) conf_matrix #&gt; pred_clas #&gt; 0 1 #&gt; 0 145 155 #&gt; 1 68 632 paste0(&quot;Akurasi Model:&quot;) #&gt; [1] &quot;Akurasi Model:&quot; accuracy &lt;- sum(diag(conf_matrix)) / sum(conf_matrix) accuracy #&gt; [1] 0.777 3.1.6 Kebaikan Model #install.packages(&quot;performance&quot;) library(performance) #&gt; Warning: package &#39;performance&#39; was built under R version #&gt; 4.4.3 #Outliers performance::check_outliers(logreg1) #&gt; OK: No outliers detected. #&gt; - Based on the following method and threshold: cook (0.8). #&gt; - For variable: (Whole model) #Metrik performance(logreg1) #&gt; # Indices of model performance #&gt; #&gt; AIC | AICc | BIC | Tjur&#39;s R2 | RMSE | Sigma #&gt; ---------------------------------------------------------- #&gt; 1025.995 | 1028.609 | 1197.767 | 0.254 | 0.395 | 1.000 #&gt; #&gt; AIC | Log_loss | Score_log | Score_spherical | PCP #&gt; --------------------------------------------------------- #&gt; 1025.995 | 0.478 | -Inf | 0.001 | 0.687 #Goodness Of Fit performance_hosmer(logreg1) #&gt; # Hosmer-Lemeshow Goodness-of-Fit Test #&gt; #&gt; Chi-squared: 8.472 #&gt; df: 8 #&gt; p-value: 0.389 #&gt; Summary: model seems to fit well. 3.2 Regresi Logistik Nominal atau Multinominal 3.2.1 Data library(readxl) students &lt;- read_excel(&quot;Data/students.xlsx&quot;) head(students,10) #&gt; # A tibble: 10 × 6 #&gt; gender ses prog read write math #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female low vocation 34 35 41 #&gt; 2 male middle general 34 33 41 #&gt; 3 male high vocation 39 39 44 #&gt; 4 male low vocation 37 37 42 #&gt; 5 male middle vocation 39 31 40 #&gt; 6 female high general 42 36 42 #&gt; 7 male middle vocation 31 36 46 #&gt; 8 male middle vocation 50 31 40 #&gt; 9 female middle vocation 39 41 33 #&gt; 10 male middle vocation 34 37 46 str(students) #&gt; tibble [200 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ gender: chr [1:200] &quot;female&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; ... #&gt; $ ses : chr [1:200] &quot;low&quot; &quot;middle&quot; &quot;high&quot; &quot;low&quot; ... #&gt; $ prog : chr [1:200] &quot;vocation&quot; &quot;general&quot; &quot;vocation&quot; &quot;vocation&quot; ... #&gt; $ read : num [1:200] 34 34 39 37 39 42 31 50 39 34 ... #&gt; $ write : num [1:200] 35 33 39 37 31 36 36 31 41 37 ... #&gt; $ math : num [1:200] 41 41 44 42 40 42 46 40 33 46 ... 3.2.2 Ubah jadi faktor library(dplyr) students &lt;- students %&gt;% mutate(across(-c(read,write,math),as.factor)) students$prog2 &lt;- relevel(students$prog, ref = &quot;academic&quot;) str(students) #&gt; tibble [200 × 7] (S3: tbl_df/tbl/data.frame) #&gt; $ gender: Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 1 2 2 1 2 ... #&gt; $ ses : Factor w/ 3 levels &quot;high&quot;,&quot;low&quot;,&quot;middle&quot;: 2 3 1 2 3 1 3 3 3 3 ... #&gt; $ prog : Factor w/ 3 levels &quot;academic&quot;,&quot;general&quot;,..: 3 2 3 3 3 2 3 3 3 3 ... #&gt; $ read : num [1:200] 34 34 39 37 39 42 31 50 39 34 ... #&gt; $ write : num [1:200] 35 33 39 37 31 36 36 31 41 37 ... #&gt; $ math : num [1:200] 41 41 44 42 40 42 46 40 33 46 ... #&gt; $ prog2 : Factor w/ 3 levels &quot;academic&quot;,&quot;general&quot;,..: 3 2 3 3 3 2 3 3 3 3 ... table(students$ses, students$prog) #&gt; #&gt; academic general vocation #&gt; high 42 9 7 #&gt; low 19 16 12 #&gt; middle 44 20 31 table(students$gender, students$prog) #&gt; #&gt; academic general vocation #&gt; female 58 24 27 #&gt; male 47 21 23 3.2.3 Pemodelan #install.packages(&quot;nnet&quot;) library(nnet) logmultinom &lt;- multinom(prog2 ~ ses + gender + write + read, data = students) #&gt; # weights: 21 (12 variable) #&gt; initial value 219.722458 #&gt; iter 10 value 176.754587 #&gt; final value 174.725397 #&gt; converged summary(logmultinom) #&gt; Call: #&gt; multinom(formula = prog2 ~ ses + gender + write + read, data = students) #&gt; #&gt; Coefficients: #&gt; (Intercept) seslow sesmiddle gendermale #&gt; general 2.621831 1.0038426 0.5651588 0.1273914 #&gt; vocation 6.505182 0.6239396 1.1539447 -0.3105237 #&gt; write read #&gt; general -0.02860308 -0.04730781 #&gt; vocation -0.08243508 -0.07108839 #&gt; #&gt; Std. Errors: #&gt; (Intercept) seslow sesmiddle gendermale #&gt; general 1.434514 0.5323398 0.4713812 0.4137756 #&gt; vocation 1.524572 0.6200276 0.5231819 0.4414783 #&gt; write read #&gt; general 0.02686316 0.02480868 #&gt; vocation 0.02793343 0.02752520 #&gt; #&gt; Residual Deviance: 349.4508 #&gt; AIC: 373.4508 z &lt;- summary(logmultinom)$coefficients/summary(logmultinom)$standard.errors # 2-tailed z test p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2 p #&gt; (Intercept) seslow sesmiddle gendermale #&gt; general 6.759775e-02 0.05933302 0.23055043 0.7581770 #&gt; vocation 1.982164e-05 0.31426675 0.02741006 0.4818237 #&gt; write read #&gt; general 0.286980200 0.056532815 #&gt; vocation 0.003166173 0.009804037 3.2.4 Odds Ratio exp(coef(logmultinom)) #&gt; (Intercept) seslow sesmiddle gendermale #&gt; general 13.7609 2.728747 1.759727 1.135862 #&gt; vocation 668.5973 1.866266 3.170676 0.733063 #&gt; write read #&gt; general 0.9718021 0.9537938 #&gt; vocation 0.9208712 0.9313796 3.2.5 Multikolineratitas library(car) vif(logmultinom) #&gt; Warning in vif.default(logmultinom): No intercept: vifs may #&gt; not be sensible. #&gt; GVIF Df GVIF^(1/(2*Df)) #&gt; ses 6.640420 2 1.605273 #&gt; gender 2.650955 1 1.628175 #&gt; write 66.396002 1 8.148374 #&gt; read 53.940932 1 7.344449 3.2.6 Akurasi df &lt;- students[,c(&quot;ses&quot;,&quot;gender&quot;,&quot;write&quot;,&quot;read&quot;)] #install.packages(&quot;caret&quot;) library(caret) #&gt; Loading required package: ggplot2 #&gt; Warning: package &#39;ggplot2&#39; was built under R version 4.4.3 #&gt; Loading required package: lattice prediksi &lt;- predict(logmultinom, df, type = &quot;class&quot;) confusionMatrix(as.factor(prediksi), students$prog2) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction academic general vocation #&gt; academic 90 25 21 #&gt; general 3 7 4 #&gt; vocation 12 13 25 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.61 #&gt; 95% CI : (0.5387, 0.678) #&gt; No Information Rate : 0.525 #&gt; P-Value [Acc &gt; NIR] : 0.009485 #&gt; #&gt; Kappa : 0.3094 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 1.959e-05 #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: academic Class: general #&gt; Sensitivity 0.8571 0.1556 #&gt; Specificity 0.5158 0.9548 #&gt; Pos Pred Value 0.6618 0.5000 #&gt; Neg Pred Value 0.7656 0.7957 #&gt; Prevalence 0.5250 0.2250 #&gt; Detection Rate 0.4500 0.0350 #&gt; Detection Prevalence 0.6800 0.0700 #&gt; Balanced Accuracy 0.6865 0.5552 #&gt; Class: vocation #&gt; Sensitivity 0.5000 #&gt; Specificity 0.8333 #&gt; Pos Pred Value 0.5000 #&gt; Neg Pred Value 0.8333 #&gt; Prevalence 0.2500 #&gt; Detection Rate 0.1250 #&gt; Detection Prevalence 0.2500 #&gt; Balanced Accuracy 0.6667 3.2.7 Kebaikan Model logmultinom0 &lt;- multinom(prog2 ~ 1, data = students) #&gt; # weights: 6 (2 variable) #&gt; initial value 219.722458 #&gt; final value 204.096674 #&gt; converged #install.packages(&quot;lmtest&quot;) library(lmtest) #&gt; Loading required package: zoo #&gt; #&gt; Attaching package: &#39;zoo&#39; #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; as.Date, as.Date.numeric lrtest(logmultinom0,logmultinom) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: prog2 ~ 1 #&gt; Model 2: prog2 ~ ses + gender + write + read #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 2 -204.10 #&gt; 2 12 -174.72 10 58.743 6.263e-09 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.3 Regresi Logistik Ordinal 3.3.1 Data crash &lt;- read.csv(&quot;Data/crash.csv&quot;) head(crash,10) #&gt; Gender Location SeatBelt Respon #&gt; 1 Female Urban Yes 1 #&gt; 2 Male Urban Yes 1 #&gt; 3 Male Urban No 1 #&gt; 4 Female Urban No 1 #&gt; 5 Male Rural Yes 1 #&gt; 6 Female Rural Yes 1 #&gt; 7 Male Rural No 1 #&gt; 8 Female Rural No 1 #&gt; 9 Female Urban No 3 #&gt; 10 Female Rural No 3 library(dplyr) crash &lt;- crash %&gt;% mutate(across(-c(Respon),as.factor)) str(crash) #&gt; &#39;data.frame&#39;: 80 obs. of 4 variables: #&gt; $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 1 2 1 2 1 1 1 ... #&gt; $ Location: Factor w/ 2 levels &quot;Rural&quot;,&quot;Urban&quot;: 2 2 2 2 1 1 1 1 2 1 ... #&gt; $ SeatBelt: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 1 1 2 2 1 1 1 1 ... #&gt; $ Respon : int 1 1 1 1 1 1 1 1 3 3 ... crash$Respon &lt;- ordered(crash$Respon, levels=c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;)) str(crash) #&gt; &#39;data.frame&#39;: 80 obs. of 4 variables: #&gt; $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 1 2 1 2 1 1 1 ... #&gt; $ Location: Factor w/ 2 levels &quot;Rural&quot;,&quot;Urban&quot;: 2 2 2 2 1 1 1 1 2 1 ... #&gt; $ SeatBelt: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 1 1 2 2 1 1 1 1 ... #&gt; $ Respon : Ord.factor w/ 5 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 1 1 1 1 1 1 1 1 3 3 ... 3.3.2 Pemodelan #install.packages(&quot;MASS&quot;) library(MASS) #&gt; #&gt; Attaching package: &#39;MASS&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; select orderlog &lt;- polr(Respon~., method=&#39;logistic&#39;,data=crash) summary(orderlog) #&gt; #&gt; Re-fitting to get Hessian #&gt; Call: #&gt; polr(formula = Respon ~ ., data = crash, method = &quot;logistic&quot;) #&gt; #&gt; Coefficients: #&gt; Value Std. Error t value #&gt; GenderMale -0.05369 0.3974 -0.1351 #&gt; LocationUrban 0.05661 0.3958 0.1430 #&gt; SeatBeltYes -0.31102 0.3974 -0.7827 #&gt; #&gt; Intercepts: #&gt; Value Std. Error t value #&gt; 1|2 -1.5425 0.4450 -3.4664 #&gt; 2|3 -0.5523 0.4060 -1.3603 #&gt; 3|4 0.2649 0.3966 0.6678 #&gt; 4|5 1.2472 0.4264 2.9249 #&gt; #&gt; Residual Deviance: 256.8444 #&gt; AIC: 270.8444 3.3.3 Odds Ratio koefisien&lt;-coef(summary(orderlog)) #&gt; #&gt; Re-fitting to get Hessian exp(koefisien[,1]) #&gt; GenderMale LocationUrban SeatBeltYes 1|2 #&gt; 0.9477303 1.0582414 0.7327004 0.2138542 #&gt; 2|3 3|4 4|5 #&gt; 0.5756362 1.3032362 3.4805710 # menghitung pvalue p &lt;- pnorm(abs(koefisien[,&quot;t value&quot;]), lower.tail = FALSE)*2 (ctabel&lt;-cbind(round(koefisien,2), &quot;pvalue&quot;=round(p,3))) #&gt; Value Std. Error t value pvalue #&gt; GenderMale -0.05 0.40 -0.14 0.893 #&gt; LocationUrban 0.06 0.40 0.14 0.886 #&gt; SeatBeltYes -0.31 0.40 -0.78 0.434 #&gt; 1|2 -1.54 0.44 -3.47 0.001 #&gt; 2|3 -0.55 0.41 -1.36 0.174 #&gt; 3|4 0.26 0.40 0.67 0.504 #&gt; 4|5 1.25 0.43 2.92 0.003 3.3.4 Multikolineratitas library(car) vif(orderlog) #&gt; #&gt; Re-fitting to get Hessian #&gt; Gender Location SeatBelt #&gt; 1.002035 1.001265 1.001814 3.3.5 Akurasi df &lt;- crash[,1:3] prediksi &lt;- predict(orderlog, df, type = &quot;class&quot;) confusionMatrix(as.factor(prediksi), crash$Respon) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 1 2 3 4 5 #&gt; 1 10 8 7 7 8 #&gt; 2 0 0 0 0 0 #&gt; 3 0 0 0 0 0 #&gt; 4 0 0 0 0 0 #&gt; 5 6 8 9 9 8 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.225 #&gt; 95% CI : (0.1391, 0.3321) #&gt; No Information Rate : 0.2 #&gt; P-Value [Acc &gt; NIR] : 0.3292 #&gt; #&gt; Kappa : 0.0312 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: 1 Class: 2 Class: 3 Class: 4 #&gt; Sensitivity 0.6250 0.0 0.0 0.0 #&gt; Specificity 0.5312 1.0 1.0 1.0 #&gt; Pos Pred Value 0.2500 NaN NaN NaN #&gt; Neg Pred Value 0.8500 0.8 0.8 0.8 #&gt; Prevalence 0.2000 0.2 0.2 0.2 #&gt; Detection Rate 0.1250 0.0 0.0 0.0 #&gt; Detection Prevalence 0.5000 0.0 0.0 0.0 #&gt; Balanced Accuracy 0.5781 0.5 0.5 0.5 #&gt; Class: 5 #&gt; Sensitivity 0.5 #&gt; Specificity 0.5 #&gt; Pos Pred Value 0.2 #&gt; Neg Pred Value 0.8 #&gt; Prevalence 0.2 #&gt; Detection Rate 0.1 #&gt; Detection Prevalence 0.5 #&gt; Balanced Accuracy 0.5 3.3.6 Kebaikan Model orderlog0 &lt;-polr(Respon~1, method = &quot;logistic&quot;, data = crash) #install.packages(&quot;lmtest&quot;) library(lmtest) lrtest(orderlog0,orderlog) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Respon ~ 1 #&gt; Model 2: Respon ~ Gender + Location + SeatBelt #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 4 -128.75 #&gt; 2 7 -128.42 3 0.6657 0.8813 "],["discriminant-analysis.html", "Chapter 4 Discriminant Analysis 4.1 Analisis Diskriminan Dua Grup 4.2 Analisis Diskriminan Tiga Grup", " Chapter 4 Discriminant Analysis 4.1 Analisis Diskriminan Dua Grup 4.1.1 Data library(readxl) pinjaman &lt;- read_excel(&quot;Data/pinjaman.xlsx&quot;) head(pinjaman,10) #&gt; # A tibble: 10 × 6 #&gt; X1 X2 X3 X4 X5 Y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 98 35 12 4 4 1 #&gt; 2 65 44 5 3 1 1 #&gt; 3 22 50 0 2 7 1 #&gt; 4 78 60 34 5 5 1 #&gt; 5 50 31 4 2 2 1 #&gt; 6 21 30 5 3 7 1 #&gt; 7 42 32 21 4 11 1 #&gt; 8 20 41 10 2 3 1 #&gt; 9 33 25 0 3 6 1 #&gt; 10 57 32 8 2 5 1 str(pinjaman) #&gt; tibble [32 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ X1: num [1:32] 98 65 22 78 50 21 42 20 33 57 ... #&gt; $ X2: num [1:32] 35 44 50 60 31 30 32 41 25 32 ... #&gt; $ X3: num [1:32] 12 5 0 34 4 5 21 10 0 8 ... #&gt; $ X4: num [1:32] 4 3 2 5 2 3 4 2 3 2 ... #&gt; $ X5: num [1:32] 4 1 7 5 2 7 11 3 6 5 ... #&gt; $ Y : num [1:32] 1 1 1 1 1 1 1 1 1 1 ... pinjaman$Y &lt;- as.factor(pinjaman$Y) str(pinjaman) #&gt; tibble [32 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ X1: num [1:32] 98 65 22 78 50 21 42 20 33 57 ... #&gt; $ X2: num [1:32] 35 44 50 60 31 30 32 41 25 32 ... #&gt; $ X3: num [1:32] 12 5 0 34 4 5 21 10 0 8 ... #&gt; $ X4: num [1:32] 4 3 2 5 2 3 4 2 3 2 ... #&gt; $ X5: num [1:32] 4 1 7 5 2 7 11 3 6 5 ... #&gt; $ Y : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... library(psych) pairs.panels(pinjaman[1:5], gap = 0, bg = c(&quot;red&quot;, &quot;green&quot;)[pinjaman$Y], pch = 21) 4.1.2 Pemodelan Linier library(MASS) modellda1 &lt;- lda(Y ~ X1 + X2 + X3 + X4 + X5, data=pinjaman) modellda1 #&gt; Call: #&gt; lda(Y ~ X1 + X2 + X3 + X4 + X5, data = pinjaman) #&gt; #&gt; Prior probabilities of groups: #&gt; 0 1 #&gt; 0.4375 0.5625 #&gt; #&gt; Group means: #&gt; X1 X2 X3 X4 X5 #&gt; 0 23.07143 26.78571 23.21429 3.428571 4.071429 #&gt; 1 44.33333 34.38889 11.72222 2.888889 4.500000 #&gt; #&gt; Coefficients of linear discriminants: #&gt; LD1 #&gt; X1 0.037015853 #&gt; X2 -0.004820049 #&gt; X3 -0.043555291 #&gt; X4 -0.477408359 #&gt; X5 -0.008483836 4.1.3 Uji Signifikansi Fungsi Diskriminan m &lt;- manova(cbind(pinjaman$X1,pinjaman$X2,pinjaman$X3, pinjaman$X4,pinjaman$X5) ~ pinjaman$Y) summary(m, test = &#39;Wilks&#39;) #&gt; Df Wilks approx F num Df den Df Pr(&gt;F) #&gt; pinjaman$Y 1 0.62715 3.0915 5 26 0.02544 * #&gt; Residuals 30 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.1.4 Akurasi p &lt;- predict(modellda1, pinjaman) ldahist(data = p$x, g = pinjaman$Y) library(caret) #&gt; Loading required package: ggplot2 #&gt; Warning: package &#39;ggplot2&#39; was built under R version 4.4.3 #&gt; #&gt; Attaching package: &#39;ggplot2&#39; #&gt; The following objects are masked from &#39;package:psych&#39;: #&gt; #&gt; %+%, alpha #&gt; Loading required package: lattice confusionMatrix(p$class,pinjaman$Y) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 0 1 #&gt; 0 9 4 #&gt; 1 5 14 #&gt; #&gt; Accuracy : 0.7188 #&gt; 95% CI : (0.5325, 0.8625) #&gt; No Information Rate : 0.5625 #&gt; P-Value [Acc &gt; NIR] : 0.0523 #&gt; #&gt; Kappa : 0.424 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 1.0000 #&gt; #&gt; Sensitivity : 0.6429 #&gt; Specificity : 0.7778 #&gt; Pos Pred Value : 0.6923 #&gt; Neg Pred Value : 0.7368 #&gt; Prevalence : 0.4375 #&gt; Detection Rate : 0.2812 #&gt; Detection Prevalence : 0.4062 #&gt; Balanced Accuracy : 0.7103 #&gt; #&gt; &#39;Positive&#39; Class : 0 #&gt; mean(p$class==pinjaman$Y) #&gt; [1] 0.71875 #install.packages(&quot;klaR&quot;) library(klaR) #&gt; Warning: package &#39;klaR&#39; was built under R version 4.4.3 #Partition plot partimat(Y~., data = pinjaman, method = &quot;lda&quot;) partimat(Y~., data = pinjaman, method = &quot;qda&quot;) 4.1.5 Pemodelan Quadratik modellda2 &lt;- qda(Y ~ X1 + X2 + X3 + X4 + X5, data=pinjaman) modellda2 #&gt; Call: #&gt; qda(Y ~ X1 + X2 + X3 + X4 + X5, data = pinjaman) #&gt; #&gt; Prior probabilities of groups: #&gt; 0 1 #&gt; 0.4375 0.5625 #&gt; #&gt; Group means: #&gt; X1 X2 X3 X4 X5 #&gt; 0 23.07143 26.78571 23.21429 3.428571 4.071429 #&gt; 1 44.33333 34.38889 11.72222 2.888889 4.500000 p &lt;- predict(modellda2, pinjaman) mean(p$class==pinjaman$Y) #&gt; [1] 0.84375 4.1.6 Tipe Diskriminan Lainnya # Mixture discriminant analysis - MDA # install.packages(&quot;mda&quot;) library(mda) #&gt; Warning: package &#39;mda&#39; was built under R version 4.4.3 #&gt; Loading required package: class #&gt; Loaded mda 0.5-5 modellda3 &lt;- mda(Y ~ X1 + X2 + X3 + X4 + X5, data=pinjaman) p &lt;- predict(modellda3, pinjaman) mean(p==pinjaman$Y) #&gt; [1] 0.84375 # Flexible discriminant analysis - FDA modellda4 &lt;- fda(Y ~ X1 + X2 + X3 + X4 + X5, data=pinjaman) p &lt;- predict(modellda4, pinjaman) mean(p==pinjaman$Y) #&gt; [1] 0.71875 # Regularized discriminant analysis - RDA modellda5 &lt;- rda(Y ~ X1 + X2 + X3 + X4 + X5, data=pinjaman) p &lt;- predict(modellda5, pinjaman) mean(p$class==pinjaman$Y) #&gt; [1] 0.75 4.2 Analisis Diskriminan Tiga Grup 4.2.1 Data data(&quot;iris&quot;) head(iris) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa str(iris) #&gt; &#39;data.frame&#39;: 150 obs. of 5 variables: #&gt; $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... #&gt; $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... #&gt; $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... #&gt; $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... #&gt; $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... library(MASS) lda.iris &lt;- lda(Species ~ ., iris) lda.iris #&gt; Call: #&gt; lda(Species ~ ., data = iris) #&gt; #&gt; Prior probabilities of groups: #&gt; setosa versicolor virginica #&gt; 0.3333333 0.3333333 0.3333333 #&gt; #&gt; Group means: #&gt; Sepal.Length Sepal.Width Petal.Length #&gt; setosa 5.006 3.428 1.462 #&gt; versicolor 5.936 2.770 4.260 #&gt; virginica 6.588 2.974 5.552 #&gt; Petal.Width #&gt; setosa 0.246 #&gt; versicolor 1.326 #&gt; virginica 2.026 #&gt; #&gt; Coefficients of linear discriminants: #&gt; LD1 LD2 #&gt; Sepal.Length 0.8293776 -0.02410215 #&gt; Sepal.Width 1.5344731 -2.16452123 #&gt; Petal.Length -2.2012117 0.93192121 #&gt; Petal.Width -2.8104603 -2.83918785 #&gt; #&gt; Proportion of trace: #&gt; LD1 LD2 #&gt; 0.9912 0.0088 4.2.2 Uji Signifikansi Fungsi Diskriminan m &lt;- manova(cbind(iris$Sepal.Length,iris$Sepal.Width,iris$Petal.Length, iris$Petal.Width) ~ iris$Species) summary(m, test = &#39;Wilks&#39;) #&gt; Df Wilks approx F num Df den Df Pr(&gt;F) #&gt; iris$Species 2 0.023439 199.15 8 288 &lt; 2.2e-16 #&gt; Residuals 147 #&gt; #&gt; iris$Species *** #&gt; Residuals #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.2.3 Akurasi p &lt;- predict(lda.iris, iris) ldahist(data = p$x, g = iris$Species) table(p$class,iris$Species) #&gt; #&gt; setosa versicolor virginica #&gt; setosa 50 0 0 #&gt; versicolor 0 48 1 #&gt; virginica 0 2 49 mean(p$class==iris$Species) #&gt; [1] 0.98 4.2.4 Visualisasi library(ggplot2) lda.data &lt;- cbind(iris, p$x) ggplot(lda.data, aes(LD1, LD2)) + geom_point(aes(color = Species)) + theme_classic() 4.2.5 Pemodelan Quadratik qda.iris &lt;- qda(Species ~ ., data=iris) qda.iris #&gt; Call: #&gt; qda(Species ~ ., data = iris) #&gt; #&gt; Prior probabilities of groups: #&gt; setosa versicolor virginica #&gt; 0.3333333 0.3333333 0.3333333 #&gt; #&gt; Group means: #&gt; Sepal.Length Sepal.Width Petal.Length #&gt; setosa 5.006 3.428 1.462 #&gt; versicolor 5.936 2.770 4.260 #&gt; virginica 6.588 2.974 5.552 #&gt; Petal.Width #&gt; setosa 0.246 #&gt; versicolor 1.326 #&gt; virginica 2.026 p &lt;- predict(qda.iris, iris) mean(p$class==iris$Species) #&gt; [1] 0.98 4.2.6 Tipe Diskriminan Lainnya # Mixture discriminant analysis - MDA # install.packages(&quot;mda&quot;) library(mda) mda.iris &lt;- mda(Species ~ ., data=iris) mda.iris #&gt; Call: #&gt; mda(formula = Species ~ ., data = iris) #&gt; #&gt; Dimension: 4 #&gt; #&gt; Percent Between-Group Variance Explained: #&gt; v1 v2 v3 v4 #&gt; 95.26 97.78 99.56 100.00 #&gt; #&gt; Degrees of Freedom (per dimension): 5 #&gt; #&gt; Training Misclassification Error: 0.01333 ( N = 150 ) #&gt; #&gt; Deviance: 13.229 p &lt;- predict(mda.iris, iris) mean(p==iris$Species) #&gt; [1] 0.9866667 # Flexible discriminant analysis - FDA fda.iris &lt;- fda(Species ~ ., data=iris) fda.iris #&gt; Call: #&gt; fda(formula = Species ~ ., data = iris) #&gt; #&gt; Dimension: 2 #&gt; #&gt; Percent Between-Group Variance Explained: #&gt; v1 v2 #&gt; 99.12 100.00 #&gt; #&gt; Degrees of Freedom (per dimension): 5 #&gt; #&gt; Training Misclassification Error: 0.02 ( N = 150 ) p &lt;- predict(fda.iris, iris) mean(p==iris$Species) #&gt; [1] 0.98 # Regularized discriminant analysis - RDA rda.iris &lt;- rda(Species ~ ., data=iris) rda.iris #&gt; Call: #&gt; rda(formula = Species ~ ., data = iris) #&gt; #&gt; Regularization parameters: #&gt; gamma lambda #&gt; 0.1678610 0.4041377 #&gt; #&gt; Prior probabilities of groups: #&gt; setosa versicolor virginica #&gt; 0.3333333 0.3333333 0.3333333 #&gt; #&gt; Misclassification rate: #&gt; apparent: 2 % #&gt; cross-validated: 2 % p &lt;- predict(rda.iris, iris) mean(p$class==iris$Species) #&gt; [1] 0.98 "],["cluster-analysis.html", "Chapter 5 Cluster Analysis 5.1 Metode berhirarki 5.2 Metode tidak berhirarki - kmeans", " Chapter 5 Cluster Analysis 5.1 Metode berhirarki Ref: https://rpubs.com/odenipinedo/cluster-analysis-in-R library(readxl) Provinsi &lt;- read_excel(&quot;Data/provinsi.xlsx&quot;) Prov.scaled = scale(Provinsi[,c(4:8)]) rownames(Prov.scaled) = Provinsi$Provinsi head(Prov.scaled) #&gt; IPM UHH RLS PPK #&gt; Aceh 0.20822137 0.04044611 0.7444782 -0.62264434 #&gt; Sumut 0.20085709 -0.39282591 1.0245728 -0.11277090 #&gt; Sumbar 0.36532598 -0.23835502 0.4747574 0.01481560 #&gt; Riau 0.50033775 0.59428078 0.5162529 0.19012890 #&gt; Jambi 0.05848104 0.50762638 -0.1165536 -0.18648754 #&gt; Sumsel -0.21890679 -0.08765171 -0.2825356 -0.02582306 #&gt; Gini #&gt; Aceh -0.8089350 #&gt; Sumut -0.6516207 #&gt; Sumbar -1.2546590 #&gt; Riau -0.9138112 #&gt; Jambi -0.6778397 #&gt; Sumsel 0.1349510 ## membuat dissimilarity matrix dprov = dist(Prov.scaled, method=&quot;euclidean&quot;) c.comp = hclust(dprov, method = &quot;complete&quot;) cor(dprov , cophenetic(c.comp)) #&gt; [1] 0.7853523 c.sing = hclust(dprov, method = &quot;single&quot;) cor(dprov , cophenetic(c.sing)) #&gt; [1] 0.7905858 c.avrg = hclust(dprov, method = &quot;average&quot;) cor(dprov , cophenetic(c.avrg)) #&gt; [1] 0.8092689 c.ward = hclust(dprov, method = &quot;ward.D&quot;) cor(dprov , cophenetic(c.ward)) #&gt; [1] 0.5336018 c.ctrd = hclust(dprov, method = &quot;centroid&quot;) cor(dprov , cophenetic(c.ctrd)) #&gt; [1] 0.7700878 library(factoextra) #&gt; Loading required package: ggplot2 #&gt; Warning: package &#39;ggplot2&#39; was built under R version 4.4.3 #&gt; Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa fviz_dend(c.avrg, cex = 0.5, main = &quot;Cluster Dendrogram average linkage&quot;) #&gt; Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use #&gt; &quot;none&quot; instead as of ggplot2 3.3.4. #&gt; ℹ The deprecated feature was likely used in the factoextra #&gt; package. #&gt; Please report the issue at #&gt; &lt;https://github.com/kassambara/factoextra/issues&gt;. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_lifecycle_warnings()` to see where #&gt; this warning was generated. avg_coph &lt;- cophenetic(c.avrg) avg_clust &lt;- cutree(c.avrg, k = 4) table(avg_clust) #&gt; avg_clust #&gt; 1 2 3 4 #&gt; 26 1 1 6 fviz_dend(c.avrg, k = 4, k_colors = &quot;jco&quot;, rect = T, main = &quot;Average Linkage Cluster&quot;) library(clValid) #&gt; Warning: package &#39;clValid&#39; was built under R version 4.4.3 #&gt; Loading required package: cluster library(cluster) # internal measures internal &lt;- clValid(Prov.scaled, nClust = 2:6, clMethods = &quot;hierarchical&quot;, validation = &quot;internal&quot;, metric = &quot;euclidean&quot;, method = &quot;average&quot;) summary(internal) #&gt; #&gt; Clustering Methods: #&gt; hierarchical #&gt; #&gt; Cluster sizes: #&gt; 2 3 4 5 6 #&gt; #&gt; Validation Measures: #&gt; 2 3 4 5 6 #&gt; #&gt; hierarchical Connectivity 4.5246 10.3012 11.6345 18.3198 24.1508 #&gt; Dunn 0.3637 0.3703 0.3703 0.3224 0.3592 #&gt; Silhouette 0.4915 0.3484 0.3092 0.2567 0.3117 #&gt; #&gt; Optimal Scores: #&gt; #&gt; Score Method Clusters #&gt; Connectivity 4.5246 hierarchical 2 #&gt; Dunn 0.3703 hierarchical 3 #&gt; Silhouette 0.4915 hierarchical 2 fviz_dend(c.avrg, k = 2, k_colors = &quot;jco&quot;, rect = T, main = &quot;Average Linkage Cluster&quot;) group = cutree(c.avrg, k = 2) group #&gt; Aceh Sumut Sumbar Riau Jambi Sumsel #&gt; 1 1 1 1 1 1 #&gt; Bengkulu Lampung Babel Kepri DKI Jabar #&gt; 1 1 1 1 2 1 #&gt; Jateng DIY Jatim Banten Bali NTB #&gt; 1 2 1 1 1 1 #&gt; NTT Kalbar Kalteng Kalsel Kaltim Kaltara #&gt; 1 1 1 1 1 1 #&gt; Sulut Sulteng Sulsel Sultra Gorontalo Sulbar #&gt; 1 1 1 1 1 1 #&gt; Maluku Malut Pabar Papua #&gt; 1 1 1 1 fviz_cluster(list(data = Prov.scaled, cluster = group)) + theme_minimal() prcomp(Prov.scaled) #&gt; Standard deviations (1, .., p=5): #&gt; [1] 1.7653705 1.0227284 0.7270850 0.5299864 0.1671984 #&gt; #&gt; Rotation (n x k) = (5 x 5): #&gt; PC1 PC2 PC3 PC4 #&gt; IPM -0.5601680 -0.05311199 -0.005227509 -0.0006949187 #&gt; UHH -0.4513030 0.05646383 -0.811065327 0.2024129889 #&gt; RLS -0.4591728 -0.33781331 0.497619343 0.5648220282 #&gt; PPK -0.5069166 0.09086739 0.227624805 -0.7546468667 #&gt; Gini -0.1213811 0.93360390 0.206658283 0.2655422416 #&gt; PC5 #&gt; IPM -0.82665781 #&gt; UHH 0.30714735 #&gt; RLS 0.32923179 #&gt; PPK 0.33685862 #&gt; Gini 0.02073819 5.2 Metode tidak berhirarki - kmeans fviz_nbclust(Prov.scaled, kmeans, method = &quot;wss&quot;) fviz_nbclust(Prov.scaled, kmeans, method = &quot;silhouette&quot;) set.seed(1) km = kmeans(Prov.scaled, centers=4) km #&gt; K-means clustering with 4 clusters of sizes 5, 7, 16, 6 #&gt; #&gt; Cluster means: #&gt; IPM UHH RLS PPK Gini #&gt; 1 1.67223995 1.1202353 1.3689855 1.75840321 0.6331131 #&gt; 2 0.22785944 0.6620973 -0.1491572 0.09292014 0.9739608 #&gt; 3 -0.08819085 -0.1001318 0.1246391 -0.24567350 -0.7991029 #&gt; 4 -1.42419372 -1.4389581 -1.2991755 -0.91861351 0.4670591 #&gt; #&gt; Clustering vector: #&gt; Aceh Sumut Sumbar Riau Jambi Sumsel #&gt; 3 3 3 3 3 3 #&gt; Bengkulu Lampung Babel Kepri DKI Jabar #&gt; 3 3 3 1 1 2 #&gt; Jateng DIY Jatim Banten Bali NTB #&gt; 2 1 2 2 1 4 #&gt; NTT Kalbar Kalteng Kalsel Kaltim Kaltara #&gt; 4 3 3 3 1 3 #&gt; Sulut Sulteng Sulsel Sultra Gorontalo Sulbar #&gt; 2 3 2 2 4 4 #&gt; Maluku Malut Pabar Papua #&gt; 3 3 4 4 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 17.054859 7.933134 22.111511 7.711994 #&gt; (between_SS / total_SS = 66.8 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; #&gt; [4] &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; #&gt; [7] &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; fviz_cluster(list(data = Prov.scaled, cluster = km$cluster)) + theme_minimal() "],["pca-analysis-and-biplot.html", "Chapter 6 PCA Analysis and Biplot 6.1 PCA 6.2 Biplot", " Chapter 6 PCA Analysis and Biplot 6.1 PCA # impor data dari excel, beri nama: Provinsi library(readxl) Provinsi = read_excel(&quot;Data/provinsi.xlsx&quot;) Prov.scaled = scale(Provinsi[,c(4:8)]) round(cor(Prov.scaled),3) #&gt; IPM UHH RLS PPK Gini #&gt; IPM 1.000 0.780 0.811 0.872 0.159 #&gt; UHH 0.780 1.000 0.447 0.581 0.153 #&gt; RLS 0.811 0.447 1.000 0.637 -0.059 #&gt; PPK 0.872 0.581 0.637 1.000 0.249 #&gt; Gini 0.159 0.153 -0.059 0.249 1.000 # PCA langkah manual Prov.eigen = eigen(cov(Prov.scaled)) Prov.eigen #&gt; eigen() decomposition #&gt; $values #&gt; [1] 3.11653307 1.04597347 0.52865259 0.28088555 0.02795532 #&gt; #&gt; $vectors #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] -0.5601680 -0.05311199 0.005227509 -0.0006949187 #&gt; [2,] -0.4513030 0.05646383 0.811065327 0.2024129889 #&gt; [3,] -0.4591728 -0.33781331 -0.497619343 0.5648220282 #&gt; [4,] -0.5069166 0.09086739 -0.227624805 -0.7546468667 #&gt; [5,] -0.1213811 0.93360390 -0.206658283 0.2655422416 #&gt; [,5] #&gt; [1,] 0.82665781 #&gt; [2,] -0.30714735 #&gt; [3,] -0.32923179 #&gt; [4,] -0.33685862 #&gt; [5,] -0.02073819 Prov.eigen$values #&gt; [1] 3.11653307 1.04597347 0.52865259 0.28088555 0.02795532 Prov.eigen$values/5 #&gt; [1] 0.623306615 0.209194694 0.105730518 0.056177109 #&gt; [5] 0.005591064 cumsum(Prov.eigen$values/5) #&gt; [1] 0.6233066 0.8325013 0.9382318 0.9944089 1.0000000 Prov.pc = as.matrix(Prov.scaled) %*% Prov.eigen$vectors round(Prov.pc,3) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] -0.063 -1.072 -0.028 0.684 0.141 #&gt; [2,] -0.269 -0.998 -0.667 0.411 0.001 #&gt; [3,] -0.170 -1.363 -0.172 -0.125 0.240 #&gt; [4,] -0.771 -1.003 0.373 0.025 0.016 #&gt; [5,] -0.032 -0.585 0.653 -0.002 0.008 #&gt; [6,] 0.289 0.226 0.046 -0.122 -0.055 #&gt; [7,] 0.167 -0.380 -0.246 0.160 0.149 #&gt; [8,] 0.632 -0.498 0.644 -0.115 -0.054 #&gt; [9,] -0.057 -1.798 0.675 -1.464 -0.089 #&gt; [10,] -2.171 -0.475 -1.111 -0.280 -0.100 #&gt; [11,] -5.201 0.488 -1.517 -0.455 -0.423 #&gt; [12,] -0.699 0.908 0.818 0.388 -0.141 #&gt; [13,] -0.467 0.567 1.901 -0.226 -0.064 #&gt; [14,] -3.637 1.770 0.377 0.349 0.361 #&gt; [15,] -0.211 1.726 0.527 -0.300 0.118 #&gt; [16,] -0.763 0.414 -0.365 -0.198 0.007 #&gt; [17,] -1.962 0.494 0.024 -0.719 0.052 #&gt; [18,] 1.779 0.864 -0.537 -0.824 0.322 #&gt; [19,] 2.630 0.251 -0.136 0.131 0.011 #&gt; [20,] 1.501 -0.351 1.137 -0.243 -0.049 #&gt; [21,] 0.004 -0.801 0.195 -0.277 -0.039 #&gt; [22,] 0.104 -0.191 -0.358 -0.828 0.030 #&gt; [23,] -2.225 -0.963 0.752 0.305 0.020 #&gt; [24,] -0.162 -1.278 1.179 0.698 -0.173 #&gt; [25,] -1.101 0.544 -0.154 0.823 -0.143 #&gt; [26,] 0.847 -0.438 -0.472 0.097 0.061 #&gt; [27,] -0.276 1.813 -0.105 0.254 0.105 #&gt; [28,] -0.147 0.982 0.109 0.925 -0.004 #&gt; [29,] 1.266 1.405 -0.356 -0.169 0.136 #&gt; [30,] 2.501 -0.280 -0.787 -0.540 0.062 #&gt; [31,] 0.929 -1.487 -1.397 0.735 0.080 #&gt; [32,] 1.193 -0.966 -0.326 0.739 -0.008 #&gt; [33,] 2.735 0.936 -0.533 0.218 -0.091 #&gt; [34,] 3.806 1.539 -0.145 -0.057 -0.487 # dengan fungsi prcomp pc = prcomp(x = Prov.scaled, center=TRUE, scale=TRUE) summary(pc) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; Standard deviation 1.7654 1.0227 0.7271 0.52999 0.16720 #&gt; Proportion of Variance 0.6233 0.2092 0.1057 0.05618 0.00559 #&gt; Cumulative Proportion 0.6233 0.8325 0.9382 0.99441 1.00000 round(pc$x,3)#scores #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; [1,] -0.063 -1.072 0.028 0.684 -0.141 #&gt; [2,] -0.269 -0.998 0.667 0.411 -0.001 #&gt; [3,] -0.170 -1.363 0.172 -0.125 -0.240 #&gt; [4,] -0.771 -1.003 -0.373 0.025 -0.016 #&gt; [5,] -0.032 -0.585 -0.653 -0.002 -0.008 #&gt; [6,] 0.289 0.226 -0.046 -0.122 0.055 #&gt; [7,] 0.167 -0.380 0.246 0.160 -0.149 #&gt; [8,] 0.632 -0.498 -0.644 -0.115 0.054 #&gt; [9,] -0.057 -1.798 -0.675 -1.464 0.089 #&gt; [10,] -2.171 -0.475 1.111 -0.280 0.100 #&gt; [11,] -5.201 0.488 1.517 -0.455 0.423 #&gt; [12,] -0.699 0.908 -0.818 0.388 0.141 #&gt; [13,] -0.467 0.567 -1.901 -0.226 0.064 #&gt; [14,] -3.637 1.770 -0.377 0.349 -0.361 #&gt; [15,] -0.211 1.726 -0.527 -0.300 -0.118 #&gt; [16,] -0.763 0.414 0.365 -0.198 -0.007 #&gt; [17,] -1.962 0.494 -0.024 -0.719 -0.052 #&gt; [18,] 1.779 0.864 0.537 -0.824 -0.322 #&gt; [19,] 2.630 0.251 0.136 0.131 -0.011 #&gt; [20,] 1.501 -0.351 -1.137 -0.243 0.049 #&gt; [21,] 0.004 -0.801 -0.195 -0.277 0.039 #&gt; [22,] 0.104 -0.191 0.358 -0.828 -0.030 #&gt; [23,] -2.225 -0.963 -0.752 0.305 -0.020 #&gt; [24,] -0.162 -1.278 -1.179 0.698 0.173 #&gt; [25,] -1.101 0.544 0.154 0.823 0.143 #&gt; [26,] 0.847 -0.438 0.472 0.097 -0.061 #&gt; [27,] -0.276 1.813 0.105 0.254 -0.105 #&gt; [28,] -0.147 0.982 -0.109 0.925 0.004 #&gt; [29,] 1.266 1.405 0.356 -0.169 -0.136 #&gt; [30,] 2.501 -0.280 0.787 -0.540 -0.062 #&gt; [31,] 0.929 -1.487 1.397 0.735 -0.080 #&gt; [32,] 1.193 -0.966 0.326 0.739 0.008 #&gt; [33,] 2.735 0.936 0.533 0.218 0.091 #&gt; [34,] 3.806 1.539 0.145 -0.057 0.487 round(pc$rotation,3) #loadings #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; IPM -0.560 -0.053 -0.005 -0.001 -0.827 #&gt; UHH -0.451 0.056 -0.811 0.202 0.307 #&gt; RLS -0.459 -0.338 0.498 0.565 0.329 #&gt; PPK -0.507 0.091 0.228 -0.755 0.337 #&gt; Gini -0.121 0.934 0.207 0.266 0.021 plot(pc) screeplot(x = pc, type=&quot;line&quot;, main=&quot;Scree plot&quot;) # korelasi variabel asli dengan PC data = cbind(Prov.pc, Prov.scaled) korelasi = cor(data) korelasi[6:10,1:2] #&gt; #&gt; IPM -0.9889040 -0.05431915 #&gt; UHH -0.7967169 0.05774717 #&gt; RLS -0.8106101 -0.34549128 #&gt; PPK -0.8948956 0.09293267 #&gt; Gini -0.2142826 0.95482326 6.2 Biplot # biplot library(factoextra) #&gt; Loading required package: ggplot2 #&gt; Warning: package &#39;ggplot2&#39; was built under R version 4.4.3 #&gt; Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa fviz_pca(pc) # alternatif bentuk biplot # install.packages(&quot;remotes&quot;) # remotes::install_github(&quot;vqv/ggbiplot&quot;) library(ggbiplot) #&gt; Warning: package &#39;ggbiplot&#39; was built under R version 4.4.3 ggbiplot(pc) biplot = ggbiplot(pcobj = pc, choices = c(1,2), obs.scale = 1, var.scale = 1, labels = row.names(Provinsi), varname.size = 3, varname.abbrev = FALSE, var.axes = TRUE, group = Provinsi$Region) biplot biplot2 = biplot + theme_bw() + theme(legend.position=&quot;bottom&quot;) + labs( title = &quot;PCA Indikator Kualitas Hidup Provinsi&quot;, color = &quot;Region&quot;) biplot2 "],["factor-analysis-and-structural-equation-modeling-sem.html", "Chapter 7 Factor Analysis and Structural Equation Modeling (SEM) 7.1 Analisis Faktor 7.2 Model Persamaan Struktural (SEM) 7.3 PLS SEM", " Chapter 7 Factor Analysis and Structural Equation Modeling (SEM) 7.1 Analisis Faktor harga &lt;- read.csv(&quot;Data/harga.csv&quot;) head(harga) #&gt; City Bread Burger Milk Oranges Tomatoes #&gt; 1 Atlanta 24.5 94.5 73.9 80.1 41.6 #&gt; 2 Baltimore 26.5 91.0 67.5 74.6 33.3 #&gt; 3 Boston 29.7 100.8 61.4 104.0 59.6 #&gt; 4 Buffalo 22.8 86.6 65.3 118.4 61.2 #&gt; 5 Chicago 26.7 86.7 62.7 105.9 60.2 #&gt; 6 Cincinnati 25.3 102.5 63.3 99.3 45.6 str(harga) #&gt; &#39;data.frame&#39;: 23 obs. of 6 variables: #&gt; $ City : chr &quot;Atlanta &quot; &quot;Baltimore &quot; &quot;Boston &quot; &quot;Buffalo &quot; ... #&gt; $ Bread : num 24.5 26.5 29.7 22.8 26.7 25.3 22.8 23.3 24.1 29.3 ... #&gt; $ Burger : num 94.5 91 100.8 86.6 86.7 ... #&gt; $ Milk : num 73.9 67.5 61.4 65.3 62.7 63.3 52.4 62.5 51.5 80.2 ... #&gt; $ Oranges : num 80.1 74.6 104 118.4 105.9 ... #&gt; $ Tomatoes: num 41.6 33.3 59.6 61.2 60.2 45.6 60.1 60.8 60.5 71.7 ... 7.1.1 EFA library(corrplot) #&gt; Warning: package &#39;corrplot&#39; was built under R version 4.4.3 #&gt; corrplot 0.95 loaded corrplot(cor(harga[,2:6]), method=&quot;number&quot;) library(psych) KMO(harga[,2:6]) #&gt; Kaiser-Meyer-Olkin factor adequacy #&gt; Call: KMO(r = harga[, 2:6]) #&gt; Overall MSA = 0.52 #&gt; MSA for each item = #&gt; Bread Burger Milk Oranges Tomatoes #&gt; 0.52 0.58 0.59 0.49 0.48 # Bartlett&#39;s Test of Sphericity cortest.bartlett(harga[,2:6]) #&gt; R was not square, finding R from data #&gt; $chisq #&gt; [1] 36.46285 #&gt; #&gt; $p.value #&gt; [1] 7.006877e-05 #&gt; #&gt; $df #&gt; [1] 10 # Anti image correlation (AIC) corrplot(KMO(harga[,2:6])$ImCo, method=&quot;number&quot;) # Determinan positif det(cor(harga[,2:6])) #&gt; [1] 0.1541406 # Principal component analysis (PCA) pca1 = princomp(harga[,2:6], scores=TRUE, cor=TRUE) summary(pca1) #&gt; Importance of components: #&gt; Comp.1 Comp.2 Comp.3 #&gt; Standard deviation 1.4841538 1.2325047 0.8824610 #&gt; Proportion of Variance 0.4405425 0.3038136 0.1557475 #&gt; Cumulative Proportion 0.4405425 0.7443561 0.9001036 #&gt; Comp.4 Comp.5 #&gt; Standard deviation 0.55357732 0.43935672 #&gt; Proportion of Variance 0.06128957 0.03860687 #&gt; Cumulative Proportion 0.96139313 1.00000000 scree(harga[,2:6]) # Menentukan faktor loading Analisis faktor loading loadings(pca1) #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; Bread 0.436 0.484 0.354 0.597 0.306 #&gt; Burger 0.542 0.292 0.307 -0.657 -0.309 #&gt; Milk 0.346 0.308 -0.866 -0.163 #&gt; Oranges 0.410 -0.579 0.108 0.399 -0.571 #&gt; Tomatoes 0.478 -0.500 -0.137 -0.211 0.677 #&gt; #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; SS loadings 1.0 1.0 1.0 1.0 1.0 #&gt; Proportion Var 0.2 0.2 0.2 0.2 0.2 #&gt; Cumulative Var 0.2 0.4 0.6 0.8 1.0 # Rotasi untuk mengkonfirmasi hasil analisis loading fa1 = factanal(harga[,2:6], factor=2, rotation=&quot;varimax&quot;) fa1 #&gt; #&gt; Call: #&gt; factanal(x = harga[, 2:6], factors = 2, rotation = &quot;varimax&quot;) #&gt; #&gt; Uniquenesses: #&gt; Bread Burger Milk Oranges Tomatoes #&gt; 0.239 0.318 0.830 0.420 0.005 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; Bread 0.868 #&gt; Burger 0.195 0.803 #&gt; Milk 0.135 0.390 #&gt; Oranges 0.756 #&gt; Tomatoes 0.985 0.157 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 1.605 1.583 #&gt; Proportion Var 0.321 0.317 #&gt; Cumulative Var 0.321 0.638 #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The chi square statistic is 1.16 on 1 degree of freedom. #&gt; The p-value is 0.282 # Diagram jalur hasil analisis EFA dan menampilkan faktor loading-nya fa.diagram(fa1$loadings, digits = 3) 7.1.2 CFA # Spesifikasi model attach(harga) model1 &lt;- &quot; F1 =~ Tomatoes + Oranges F2 =~ Bread + Burger + Milk F1 ~~ F2 &quot; library(lavaan) #&gt; Warning: package &#39;lavaan&#39; was built under R version 4.4.3 #&gt; This is lavaan 0.6-19 #&gt; lavaan is FREE software! Please report any bugs. #&gt; #&gt; Attaching package: &#39;lavaan&#39; #&gt; The following object is masked from &#39;package:psych&#39;: #&gt; #&gt; cor2cov fitmod = cfa(model1, data = harga) #&gt; Warning: lavaan-&gt;lav_object_post_check(): #&gt; some estimated ov variances are negative summary(fitmod, fit.measures = TRUE, standardized = TRUE) #&gt; lavaan 0.6-19 ended normally after 85 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 11 #&gt; #&gt; Number of observations 23 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 3.642 #&gt; Degrees of freedom 4 #&gt; P-value (Chi-square) 0.457 #&gt; #&gt; Model Test Baseline Model: #&gt; #&gt; Test statistic 43.007 #&gt; Degrees of freedom 10 #&gt; P-value 0.000 #&gt; #&gt; User Model versus Baseline Model: #&gt; #&gt; Comparative Fit Index (CFI) 1.000 #&gt; Tucker-Lewis Index (TLI) 1.027 #&gt; #&gt; Loglikelihood and Information Criteria: #&gt; #&gt; Loglikelihood user model (H0) -367.812 #&gt; Loglikelihood unrestricted model (H1) -365.991 #&gt; #&gt; Akaike (AIC) 757.623 #&gt; Bayesian (BIC) 770.114 #&gt; Sample-size adjusted Bayesian (SABIC) 736.072 #&gt; #&gt; Root Mean Square Error of Approximation: #&gt; #&gt; RMSEA 0.000 #&gt; 90 Percent confidence interval - lower 0.000 #&gt; 90 Percent confidence interval - upper 0.302 #&gt; P-value H_0: RMSEA &lt;= 0.050 0.487 #&gt; P-value H_0: RMSEA &gt;= 0.080 0.469 #&gt; #&gt; Standardized Root Mean Square Residual: #&gt; #&gt; SRMR 0.065 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Expected #&gt; Information saturated (h1) model Structured #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; F1 =~ #&gt; Tomatoes 1.000 #&gt; Oranges 0.934 0.580 1.611 0.107 #&gt; F2 =~ #&gt; Bread 1.000 #&gt; Burger 4.700 2.464 1.907 0.056 #&gt; Milk 1.307 0.858 1.523 0.128 #&gt; Std.lv Std.all #&gt; #&gt; 10.659 1.062 #&gt; 9.952 0.715 #&gt; #&gt; 1.622 0.662 #&gt; 7.623 1.032 #&gt; 2.119 0.312 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; F1 ~~ #&gt; F2 5.161 4.482 1.151 0.250 #&gt; Std.lv Std.all #&gt; #&gt; 0.299 0.299 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .Tomatoes -12.966 66.742 -0.194 0.846 #&gt; .Oranges 94.906 64.476 1.472 0.141 #&gt; .Bread 3.381 1.581 2.138 0.033 #&gt; .Burger -3.518 27.199 -0.129 0.897 #&gt; .Milk 41.714 12.439 3.354 0.001 #&gt; F1 113.605 72.842 1.560 0.119 #&gt; F2 2.631 1.912 1.376 0.169 #&gt; Std.lv Std.all #&gt; -12.966 -0.129 #&gt; 94.906 0.489 #&gt; 3.381 0.562 #&gt; -3.518 -0.064 #&gt; 41.714 0.903 #&gt; 1.000 1.000 #&gt; 1.000 1.000 fitmeasures(fitmod) #&gt; npar fmin #&gt; 11.000 0.079 #&gt; chisq df #&gt; 3.642 4.000 #&gt; pvalue baseline.chisq #&gt; 0.457 43.007 #&gt; baseline.df baseline.pvalue #&gt; 10.000 0.000 #&gt; cfi tli #&gt; 1.000 1.027 #&gt; nnfi rfi #&gt; 1.027 0.788 #&gt; nfi pnfi #&gt; 0.915 0.366 #&gt; ifi rni #&gt; 1.009 1.011 #&gt; logl unrestricted.logl #&gt; -367.812 -365.991 #&gt; aic bic #&gt; 757.623 770.114 #&gt; ntotal bic2 #&gt; 23.000 736.072 #&gt; rmsea rmsea.ci.lower #&gt; 0.000 0.000 #&gt; rmsea.ci.upper rmsea.ci.level #&gt; 0.302 0.900 #&gt; rmsea.pvalue rmsea.close.h0 #&gt; 0.487 0.050 #&gt; rmsea.notclose.pvalue rmsea.notclose.h0 #&gt; 0.469 0.080 #&gt; rmr rmr_nomean #&gt; 2.823 2.823 #&gt; srmr srmr_bentler #&gt; 0.065 0.065 #&gt; srmr_bentler_nomean crmr #&gt; 0.065 0.080 #&gt; crmr_nomean srmr_mplus #&gt; 0.080 0.065 #&gt; srmr_mplus_nomean cn_05 #&gt; 0.065 60.915 #&gt; cn_01 gfi #&gt; 84.843 0.947 #&gt; agfi pgfi #&gt; 0.803 0.253 #&gt; mfi ecvi #&gt; 1.008 1.115 library(semPlot) #&gt; Warning: package &#39;semPlot&#39; was built under R version 4.4.3 semPaths(fitmod, what=&#39;std&#39;, layout=&#39;tree&#39;, title = TRUE, posCol = 1, nDigits = 3, edge.label.cex=0.7, exoVar = FALSE, sizeMan = 5, sizeLat = 5) # Estimasi Reliabilitas alpha cronbach psych::alpha(harga[,2:6]) #&gt; Number of categories should be increased in order to count frequencies. #&gt; #&gt; Reliability analysis #&gt; Call: psych::alpha(x = harga[, 2:6]) #&gt; #&gt; raw_alpha std.alpha G6(smc) average_r S/N ase mean sd #&gt; 0.63 0.67 0.77 0.29 2.1 0.1 67 5.8 #&gt; median_r #&gt; 0.26 #&gt; #&gt; 95% confidence boundaries #&gt; lower alpha upper #&gt; Feldt 0.32 0.63 0.82 #&gt; Duhachek 0.42 0.63 0.83 #&gt; #&gt; Reliability if an item is dropped: #&gt; raw_alpha std.alpha G6(smc) average_r S/N alpha se #&gt; Bread 0.64 0.63 0.68 0.30 1.7 0.110 #&gt; Burger 0.56 0.54 0.63 0.23 1.2 0.107 #&gt; Milk 0.64 0.68 0.78 0.34 2.1 0.091 #&gt; Oranges 0.55 0.65 0.66 0.32 1.9 0.140 #&gt; Tomatoes 0.37 0.59 0.61 0.26 1.4 0.197 #&gt; var.r med.r #&gt; Bread 0.065 0.26 #&gt; Burger 0.083 0.13 #&gt; Milk 0.096 0.26 #&gt; Oranges 0.043 0.32 #&gt; Tomatoes 0.062 0.27 #&gt; #&gt; Item statistics #&gt; n raw.r std.r r.cor r.drop mean sd #&gt; Bread 23 0.38 0.64 0.56 0.30 25 2.5 #&gt; Burger 23 0.62 0.77 0.72 0.41 92 7.6 #&gt; Milk 23 0.42 0.56 0.36 0.20 62 7.0 #&gt; Oranges 23 0.82 0.61 0.56 0.49 103 14.2 #&gt; Tomatoes 23 0.86 0.71 0.68 0.71 52 10.3 7.2 Model Persamaan Struktural (SEM) library(lavaan) library(semPlot) library(readxl) datasem &lt;- read_excel(&quot;Data/Datalikert.xlsx&quot;) head(datasem[,1:5]) #&gt; # A tibble: 6 × 5 #&gt; Perusahaan Provinsi Pulau A1 A2 #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Jawa Barat Jawa 4 5 #&gt; 2 2 Jawa Timur Jawa 5 5 #&gt; 3 3 Jawa Timur Jawa 4 4 #&gt; 4 4 Jawa Barat Jawa 4 4 #&gt; 5 5 Jawa Timur Jawa 4 4 #&gt; 6 6 Jawa Timur Jawa 4 4 str(datasem) #&gt; tibble [300 × 45] (S3: tbl_df/tbl/data.frame) #&gt; $ Perusahaan: num [1:300] 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ Provinsi : chr [1:300] &quot;Jawa Barat&quot; &quot;Jawa Timur&quot; &quot;Jawa Timur&quot; &quot;Jawa Barat&quot; ... #&gt; $ Pulau : chr [1:300] &quot;Jawa&quot; &quot;Jawa&quot; &quot;Jawa&quot; &quot;Jawa&quot; ... #&gt; $ A1 : num [1:300] 4 5 4 4 4 4 4 5 4 5 ... #&gt; $ A2 : num [1:300] 5 5 4 4 4 4 4 5 4 5 ... #&gt; $ A3 : num [1:300] 5 5 4 3 4 5 4 5 3 5 ... #&gt; $ A4 : num [1:300] 4 5 4 4 3 4 4 5 3 5 ... #&gt; $ A5 : num [1:300] 4 4 4 4 4 4 4 5 3 5 ... #&gt; $ A6 : num [1:300] 4 5 4 4 4 4 4 5 3 4 ... #&gt; $ A7 : num [1:300] 5 5 5 4 4 4 4 5 3 5 ... #&gt; $ A8 : num [1:300] 5 5 5 4 4 4 4 5 3 4 ... #&gt; $ Atotal : num [1:300] 36 39 34 31 31 33 32 40 26 38 ... #&gt; $ B1 : num [1:300] 4 4 4 4 3 5 3 3 3 4 ... #&gt; $ B2 : num [1:300] 4 4 4 3 4 4 3 3 2 4 ... #&gt; $ Btotal : num [1:300] 8 8 8 7 7 9 6 6 5 8 ... #&gt; $ C1 : num [1:300] 4 4 4 4 4 4 4 5 3 4 ... #&gt; $ C2 : num [1:300] 4 4 4 4 4 4 4 4 3 4 ... #&gt; $ Ctotal : num [1:300] 8 8 8 8 8 8 8 9 6 8 ... #&gt; $ D1 : num [1:300] 4 5 4 4 4 4 4 4 3 4 ... #&gt; $ D2 : num [1:300] 4 5 4 3 4 5 4 4 2 4 ... #&gt; $ D3 : num [1:300] 4 5 4 4 4 4 4 4 3 4 ... #&gt; $ D4 : num [1:300] 4 5 4 5 4 4 4 4 3 4 ... #&gt; $ Dtotal : num [1:300] 16 20 16 16 16 17 16 16 11 16 ... #&gt; $ E1 : num [1:300] 5 5 4 4 4 4 4 4 3 5 ... #&gt; $ E2 : num [1:300] 5 5 4 4 4 5 4 4 3 5 ... #&gt; $ E3 : num [1:300] 5 5 4 4 4 5 4 5 4 5 ... #&gt; $ E4 : num [1:300] 4 5 4 3 4 5 4 4 3 4 ... #&gt; $ E5 : num [1:300] 4 5 4 4 3 5 4 4 3 4 ... #&gt; $ E6 : num [1:300] 4 5 4 4 4 4 4 4 3 4 ... #&gt; $ E7 : num [1:300] 4 5 4 4 4 5 4 4 3 4 ... #&gt; $ E8 : num [1:300] 4 5 4 4 3 5 4 4 3 4 ... #&gt; $ E9 : num [1:300] 4 5 4 4 4 4 4 4 3 4 ... #&gt; $ E10 : num [1:300] 4 5 4 4 4 5 4 5 3 4 ... #&gt; $ E11 : num [1:300] 4 5 4 3 3 5 4 5 3 4 ... #&gt; $ E12 : num [1:300] 5 5 4 4 4 5 4 5 3 5 ... #&gt; $ Etotal : num [1:300] 52 60 48 46 45 57 48 52 37 52 ... #&gt; $ F1 : num [1:300] 5 5 4 4 4 5 4 4 2 4 ... #&gt; $ F2 : num [1:300] 4 5 4 4 4 5 4 4 3 3 ... #&gt; $ F3 : num [1:300] 4 5 4 4 4 4 4 4 2 3 ... #&gt; $ F4 : num [1:300] 4 5 4 4 4 5 4 5 3 4 ... #&gt; $ F5 : num [1:300] 4 5 4 4 3 5 4 4 3 3 ... #&gt; $ F6 : num [1:300] 4 5 4 4 3 4 4 5 3 4 ... #&gt; $ F7 : num [1:300] 4 5 4 4 3 4 4 4 3 4 ... #&gt; $ F8 : num [1:300] 4 5 4 4 4 5 4 4 3 4 ... #&gt; $ Ftotal : num [1:300] 33 40 32 32 29 37 32 34 22 29 ... attach(datasem) table(A1) #&gt; A1 #&gt; 1 2 3 4 5 #&gt; 3 4 37 121 135 barplot(table(A1)) # Spesifikasi Model sem.model = &quot; faktor =~ A1 + A2 + A3 + A4 permintaan =~ B1 + B2 industri =~ C1 + C2 strategi =~ D1 + D2 + D3 + D4 regulasi =~ E1 + E2 + E3 + E4 + E5 + E6 kesempatan =~ F1 + F2 + F3 + F4 kesempatan ~ faktor + permintaan + industri + strategi + regulasi&quot; sem.fit = sem(sem.model, data = datasem) summary(sem.fit, fit.measures=TRUE) #&gt; lavaan 0.6-19 ended normally after 90 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 59 #&gt; #&gt; Number of observations 300 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 555.757 #&gt; Degrees of freedom 194 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Model Test Baseline Model: #&gt; #&gt; Test statistic 7355.210 #&gt; Degrees of freedom 231 #&gt; P-value 0.000 #&gt; #&gt; User Model versus Baseline Model: #&gt; #&gt; Comparative Fit Index (CFI) 0.949 #&gt; Tucker-Lewis Index (TLI) 0.940 #&gt; #&gt; Loglikelihood and Information Criteria: #&gt; #&gt; Loglikelihood user model (H0) -4608.159 #&gt; Loglikelihood unrestricted model (H1) -4330.280 #&gt; #&gt; Akaike (AIC) 9334.318 #&gt; Bayesian (BIC) 9552.841 #&gt; Sample-size adjusted Bayesian (SABIC) 9365.728 #&gt; #&gt; Root Mean Square Error of Approximation: #&gt; #&gt; RMSEA 0.079 #&gt; 90 Percent confidence interval - lower 0.071 #&gt; 90 Percent confidence interval - upper 0.087 #&gt; P-value H_0: RMSEA &lt;= 0.050 0.000 #&gt; P-value H_0: RMSEA &gt;= 0.080 0.410 #&gt; #&gt; Standardized Root Mean Square Residual: #&gt; #&gt; SRMR 0.035 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Expected #&gt; Information saturated (h1) model Structured #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; faktor =~ #&gt; A1 1.000 #&gt; A2 1.266 0.089 14.271 0.000 #&gt; A3 1.312 0.094 13.991 0.000 #&gt; A4 1.261 0.091 13.913 0.000 #&gt; permintaan =~ #&gt; B1 1.000 #&gt; B2 1.020 0.063 16.072 0.000 #&gt; industri =~ #&gt; C1 1.000 #&gt; C2 1.035 0.044 23.446 0.000 #&gt; strategi =~ #&gt; D1 1.000 #&gt; D2 0.973 0.033 29.472 0.000 #&gt; D3 0.972 0.043 22.590 0.000 #&gt; D4 0.817 0.042 19.325 0.000 #&gt; regulasi =~ #&gt; E1 1.000 #&gt; E2 0.929 0.039 23.666 0.000 #&gt; E3 0.950 0.043 22.088 0.000 #&gt; E4 1.015 0.039 25.697 0.000 #&gt; E5 0.985 0.042 23.464 0.000 #&gt; E6 0.913 0.045 20.186 0.000 #&gt; kesempatan =~ #&gt; F1 1.000 #&gt; F2 1.006 0.038 26.712 0.000 #&gt; F3 1.033 0.042 24.672 0.000 #&gt; F4 0.943 0.046 20.414 0.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; kesempatan ~ #&gt; faktor 0.016 0.111 0.146 0.884 #&gt; permintaan 0.042 0.059 0.705 0.481 #&gt; industri 0.129 0.133 0.976 0.329 #&gt; strategi 0.131 0.091 1.449 0.147 #&gt; regulasi 0.685 0.077 8.860 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; faktor ~~ #&gt; permintaan 0.233 0.034 6.785 0.000 #&gt; industri 0.327 0.037 8.729 0.000 #&gt; strategi 0.292 0.035 8.242 0.000 #&gt; regulasi 0.343 0.039 8.730 0.000 #&gt; permintaan ~~ #&gt; industri 0.366 0.043 8.447 0.000 #&gt; strategi 0.391 0.045 8.713 0.000 #&gt; regulasi 0.332 0.043 7.797 0.000 #&gt; industri ~~ #&gt; strategi 0.437 0.043 10.274 0.000 #&gt; regulasi 0.416 0.043 9.764 0.000 #&gt; strategi ~~ #&gt; regulasi 0.405 0.042 9.580 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .A1 0.323 0.029 11.229 0.000 #&gt; .A2 0.161 0.018 8.902 0.000 #&gt; .A3 0.205 0.022 9.430 0.000 #&gt; .A4 0.198 0.021 9.552 0.000 #&gt; .B1 0.269 0.032 8.457 0.000 #&gt; .B2 0.078 0.025 3.161 0.002 #&gt; .C1 0.122 0.014 8.515 0.000 #&gt; .C2 0.106 0.014 7.549 0.000 #&gt; .D1 0.093 0.011 8.749 0.000 #&gt; .D2 0.063 0.008 7.476 0.000 #&gt; .D3 0.182 0.017 10.625 0.000 #&gt; .D4 0.200 0.018 11.219 0.000 #&gt; .E1 0.145 0.014 10.563 0.000 #&gt; .E2 0.114 0.011 10.395 0.000 #&gt; .E3 0.156 0.014 10.845 0.000 #&gt; .E4 0.091 0.010 9.488 0.000 #&gt; .E5 0.133 0.013 10.462 0.000 #&gt; .E6 0.198 0.018 11.224 0.000 #&gt; .F1 0.139 0.014 9.697 0.000 #&gt; .F2 0.090 0.011 8.221 0.000 #&gt; .F3 0.140 0.015 9.540 0.000 #&gt; .F4 0.233 0.021 10.912 0.000 #&gt; faktor 0.321 0.047 6.841 0.000 #&gt; permintaan 0.525 0.065 8.048 0.000 #&gt; industri 0.480 0.049 9.751 0.000 #&gt; strategi 0.522 0.050 10.406 0.000 #&gt; regulasi 0.542 0.055 9.811 0.000 #&gt; .kesempatan 0.122 0.015 8.068 0.000 sem.fit = sem(sem.model, data = datasem, std.lv=TRUE) summary(sem.fit, fit.measures=TRUE, standardized=TRUE) #&gt; lavaan 0.6-19 ended normally after 90 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 59 #&gt; #&gt; Number of observations 300 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 555.757 #&gt; Degrees of freedom 194 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Model Test Baseline Model: #&gt; #&gt; Test statistic 7355.210 #&gt; Degrees of freedom 231 #&gt; P-value 0.000 #&gt; #&gt; User Model versus Baseline Model: #&gt; #&gt; Comparative Fit Index (CFI) 0.949 #&gt; Tucker-Lewis Index (TLI) 0.940 #&gt; #&gt; Loglikelihood and Information Criteria: #&gt; #&gt; Loglikelihood user model (H0) -4608.159 #&gt; Loglikelihood unrestricted model (H1) -4330.280 #&gt; #&gt; Akaike (AIC) 9334.318 #&gt; Bayesian (BIC) 9552.841 #&gt; Sample-size adjusted Bayesian (SABIC) 9365.728 #&gt; #&gt; Root Mean Square Error of Approximation: #&gt; #&gt; RMSEA 0.079 #&gt; 90 Percent confidence interval - lower 0.071 #&gt; 90 Percent confidence interval - upper 0.087 #&gt; P-value H_0: RMSEA &lt;= 0.050 0.000 #&gt; P-value H_0: RMSEA &gt;= 0.080 0.410 #&gt; #&gt; Standardized Root Mean Square Residual: #&gt; #&gt; SRMR 0.035 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Expected #&gt; Information saturated (h1) model Structured #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; faktor =~ #&gt; A1 0.566 0.041 13.681 0.000 #&gt; A2 0.717 0.038 18.699 0.000 #&gt; A3 0.743 0.041 18.064 0.000 #&gt; A4 0.714 0.040 17.894 0.000 #&gt; permintaan =~ #&gt; B1 0.725 0.045 16.097 0.000 #&gt; B2 0.739 0.038 19.509 0.000 #&gt; industri =~ #&gt; C1 0.692 0.036 19.503 0.000 #&gt; C2 0.717 0.036 20.132 0.000 #&gt; strategi =~ #&gt; D1 0.723 0.035 20.812 0.000 #&gt; D2 0.703 0.033 21.615 0.000 #&gt; D3 0.702 0.038 18.344 0.000 #&gt; D4 0.590 0.036 16.459 0.000 #&gt; regulasi =~ #&gt; E1 0.736 0.038 19.623 0.000 #&gt; E2 0.684 0.034 19.941 0.000 #&gt; E3 0.699 0.037 18.967 0.000 #&gt; E4 0.747 0.035 21.120 0.000 #&gt; E5 0.725 0.037 19.819 0.000 #&gt; E6 0.673 0.038 17.720 0.000 #&gt; kesempatan =~ #&gt; F1 0.350 0.022 16.135 0.000 #&gt; F2 0.352 0.021 16.722 0.000 #&gt; F3 0.361 0.022 16.227 0.000 #&gt; F4 0.330 0.022 14.833 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.566 0.706 #&gt; 0.717 0.872 #&gt; 0.743 0.854 #&gt; 0.714 0.849 #&gt; #&gt; 0.725 0.813 #&gt; 0.739 0.935 #&gt; #&gt; 0.692 0.893 #&gt; 0.717 0.911 #&gt; #&gt; 0.723 0.922 #&gt; 0.703 0.941 #&gt; 0.702 0.855 #&gt; 0.590 0.797 #&gt; #&gt; 0.736 0.888 #&gt; 0.684 0.897 #&gt; 0.699 0.870 #&gt; 0.747 0.927 #&gt; 0.725 0.894 #&gt; 0.673 0.834 #&gt; #&gt; 0.771 0.900 #&gt; 0.776 0.933 #&gt; 0.796 0.905 #&gt; 0.727 0.833 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; kesempatan ~ #&gt; faktor 0.026 0.180 0.146 0.884 #&gt; permintaan 0.086 0.123 0.705 0.481 #&gt; industri 0.256 0.263 0.973 0.331 #&gt; strategi 0.272 0.188 1.447 0.148 #&gt; regulasi 1.443 0.190 7.608 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.012 0.012 #&gt; 0.039 0.039 #&gt; 0.116 0.116 #&gt; 0.123 0.123 #&gt; 0.654 0.654 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; faktor ~~ #&gt; permintaan 0.568 0.046 12.297 0.000 #&gt; industri 0.833 0.025 33.258 0.000 #&gt; strategi 0.715 0.033 21.548 0.000 #&gt; regulasi 0.822 0.023 35.175 0.000 #&gt; permintaan ~~ #&gt; industri 0.729 0.035 20.610 0.000 #&gt; strategi 0.746 0.032 23.194 0.000 #&gt; regulasi 0.623 0.041 15.291 0.000 #&gt; industri ~~ #&gt; strategi 0.874 0.020 44.744 0.000 #&gt; regulasi 0.816 0.024 33.446 0.000 #&gt; strategi ~~ #&gt; regulasi 0.762 0.027 27.976 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.568 0.568 #&gt; 0.833 0.833 #&gt; 0.715 0.715 #&gt; 0.822 0.822 #&gt; #&gt; 0.729 0.729 #&gt; 0.746 0.746 #&gt; 0.623 0.623 #&gt; #&gt; 0.874 0.874 #&gt; 0.816 0.816 #&gt; #&gt; 0.762 0.762 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .A1 0.323 0.029 11.229 0.000 #&gt; .A2 0.161 0.018 8.902 0.000 #&gt; .A3 0.205 0.022 9.430 0.000 #&gt; .A4 0.198 0.021 9.552 0.000 #&gt; .B1 0.269 0.032 8.457 0.000 #&gt; .B2 0.078 0.025 3.161 0.002 #&gt; .C1 0.122 0.014 8.515 0.000 #&gt; .C2 0.106 0.014 7.549 0.000 #&gt; .D1 0.093 0.011 8.749 0.000 #&gt; .D2 0.063 0.008 7.476 0.000 #&gt; .D3 0.182 0.017 10.625 0.000 #&gt; .D4 0.200 0.018 11.219 0.000 #&gt; .E1 0.145 0.014 10.563 0.000 #&gt; .E2 0.114 0.011 10.395 0.000 #&gt; .E3 0.156 0.014 10.845 0.000 #&gt; .E4 0.091 0.010 9.488 0.000 #&gt; .E5 0.133 0.013 10.462 0.000 #&gt; .E6 0.198 0.018 11.224 0.000 #&gt; .F1 0.139 0.014 9.697 0.000 #&gt; .F2 0.090 0.011 8.221 0.000 #&gt; .F3 0.140 0.015 9.540 0.000 #&gt; .F4 0.233 0.021 10.912 0.000 #&gt; faktor 1.000 #&gt; permintaan 1.000 #&gt; industri 1.000 #&gt; strategi 1.000 #&gt; regulasi 1.000 #&gt; .kesempatan 1.000 #&gt; Std.lv Std.all #&gt; 0.323 0.502 #&gt; 0.161 0.239 #&gt; 0.205 0.271 #&gt; 0.198 0.280 #&gt; 0.269 0.339 #&gt; 0.078 0.126 #&gt; 0.122 0.203 #&gt; 0.106 0.171 #&gt; 0.093 0.151 #&gt; 0.063 0.114 #&gt; 0.182 0.270 #&gt; 0.200 0.365 #&gt; 0.145 0.211 #&gt; 0.114 0.195 #&gt; 0.156 0.242 #&gt; 0.091 0.141 #&gt; 0.133 0.201 #&gt; 0.198 0.304 #&gt; 0.139 0.190 #&gt; 0.090 0.130 #&gt; 0.140 0.181 #&gt; 0.233 0.306 #&gt; 1.000 1.000 #&gt; 1.000 1.000 #&gt; 1.000 1.000 #&gt; 1.000 1.000 #&gt; 1.000 1.000 #&gt; 0.206 0.206 #sem.fit = sem(sem.model, data = datasem, std.lv=TRUE, orthogonal=TRUE) #summary(sem.fit, fit.measures=TRUE, standardized=TRUE) # Modification Indices modificationIndices(sem.fit, minimum.value = 10) #&gt; lhs op rhs mi epc sepc.lv sepc.all #&gt; 72 faktor =~ D3 10.792 0.143 0.143 0.174 #&gt; 82 faktor =~ F3 14.022 -0.170 -0.170 -0.193 #&gt; 99 permintaan =~ E6 13.919 0.142 0.142 0.176 #&gt; 112 industri =~ D3 19.393 0.315 0.315 0.383 #&gt; 134 strategi =~ E3 11.975 -0.144 -0.144 -0.179 #&gt; 152 regulasi =~ D3 18.808 0.197 0.197 0.240 #&gt; 157 regulasi =~ F4 13.142 0.272 0.272 0.312 #&gt; 168 kesempatan =~ D3 22.896 0.100 0.220 0.268 #&gt; 175 kesempatan =~ E6 25.214 0.153 0.337 0.418 #&gt; 176 A1 ~~ A2 15.863 0.068 0.068 0.298 #&gt; 270 B1 ~~ F4 14.265 0.063 0.063 0.253 #&gt; 317 D1 ~~ D3 10.752 -0.035 -0.035 -0.272 #&gt; 331 D2 ~~ E1 11.098 0.025 0.025 0.257 #&gt; 347 D3 ~~ E6 12.029 0.042 0.042 0.223 #&gt; 351 D3 ~~ F4 10.217 -0.043 -0.043 -0.208 #&gt; 352 D4 ~~ E1 11.953 -0.038 -0.038 -0.223 #&gt; 362 E1 ~~ E2 17.329 0.038 0.038 0.294 #&gt; 363 E1 ~~ E3 10.360 0.033 0.033 0.220 #&gt; 364 E1 ~~ E4 12.186 -0.031 -0.031 -0.266 #&gt; 371 E2 ~~ E3 11.663 0.032 0.032 0.236 #&gt; 373 E2 ~~ E5 10.449 -0.028 -0.028 -0.231 #&gt; 381 E3 ~~ E6 11.439 -0.039 -0.039 -0.221 #&gt; 386 E4 ~~ E5 25.380 0.043 0.043 0.388 #&gt; 398 E6 ~~ F2 14.478 -0.037 -0.037 -0.275 #&gt; 399 E6 ~~ F3 20.998 0.052 0.052 0.310 #&gt; 405 F2 ~~ F4 24.019 -0.058 -0.058 -0.404 #&gt; 406 F3 ~~ F4 14.294 0.050 0.050 0.279 #&gt; sepc.nox #&gt; 72 0.174 #&gt; 82 -0.193 #&gt; 99 0.176 #&gt; 112 0.383 #&gt; 134 -0.179 #&gt; 152 0.240 #&gt; 157 0.312 #&gt; 168 0.268 #&gt; 175 0.418 #&gt; 176 0.298 #&gt; 270 0.253 #&gt; 317 -0.272 #&gt; 331 0.257 #&gt; 347 0.223 #&gt; 351 -0.208 #&gt; 352 -0.223 #&gt; 362 0.294 #&gt; 363 0.220 #&gt; 364 -0.266 #&gt; 371 0.236 #&gt; 373 -0.231 #&gt; 381 -0.221 #&gt; 386 0.388 #&gt; 398 -0.275 #&gt; 399 0.310 #&gt; 405 -0.404 #&gt; 406 0.279 sem.model2 = &quot; faktor =~ A1 + A2 + A3 + A4 permintaan =~ B1 + B2 industri =~ C1 + C2 strategi =~ D1 + D2 + D3 + D4 regulasi =~ E1 + E2 + E3 + E4 + E5 + E6 kesempatan =~ F1 + F2 + F3 + F4 kesempatan ~ faktor + permintaan + industri + strategi + regulasi A1 ~~ A2 &quot; sem.fit = sem(sem.model2, data = datasem, std.lv=TRUE) summary(sem.fit, fit.measures=TRUE, standardized=TRUE) #&gt; lavaan 0.6-19 ended normally after 94 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 60 #&gt; #&gt; Number of observations 300 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 540.535 #&gt; Degrees of freedom 193 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Model Test Baseline Model: #&gt; #&gt; Test statistic 7355.210 #&gt; Degrees of freedom 231 #&gt; P-value 0.000 #&gt; #&gt; User Model versus Baseline Model: #&gt; #&gt; Comparative Fit Index (CFI) 0.951 #&gt; Tucker-Lewis Index (TLI) 0.942 #&gt; #&gt; Loglikelihood and Information Criteria: #&gt; #&gt; Loglikelihood user model (H0) -4600.548 #&gt; Loglikelihood unrestricted model (H1) -4330.280 #&gt; #&gt; Akaike (AIC) 9321.095 #&gt; Bayesian (BIC) 9543.322 #&gt; Sample-size adjusted Bayesian (SABIC) 9353.038 #&gt; #&gt; Root Mean Square Error of Approximation: #&gt; #&gt; RMSEA 0.077 #&gt; 90 Percent confidence interval - lower 0.070 #&gt; 90 Percent confidence interval - upper 0.085 #&gt; P-value H_0: RMSEA &lt;= 0.050 0.000 #&gt; P-value H_0: RMSEA &gt;= 0.080 0.303 #&gt; #&gt; Standardized Root Mean Square Residual: #&gt; #&gt; SRMR 0.035 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Expected #&gt; Information saturated (h1) model Structured #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; faktor =~ #&gt; A1 0.539 0.043 12.660 0.000 #&gt; A2 0.702 0.039 18.009 0.000 #&gt; A3 0.752 0.041 18.363 0.000 #&gt; A4 0.720 0.040 18.060 0.000 #&gt; permintaan =~ #&gt; B1 0.724 0.045 16.093 0.000 #&gt; B2 0.739 0.038 19.507 0.000 #&gt; industri =~ #&gt; C1 0.692 0.036 19.469 0.000 #&gt; C2 0.717 0.036 20.171 0.000 #&gt; strategi =~ #&gt; D1 0.723 0.035 20.813 0.000 #&gt; D2 0.703 0.033 21.613 0.000 #&gt; D3 0.702 0.038 18.345 0.000 #&gt; D4 0.590 0.036 16.460 0.000 #&gt; regulasi =~ #&gt; E1 0.736 0.038 19.615 0.000 #&gt; E2 0.684 0.034 19.943 0.000 #&gt; E3 0.699 0.037 18.964 0.000 #&gt; E4 0.747 0.035 21.115 0.000 #&gt; E5 0.726 0.037 19.826 0.000 #&gt; E6 0.673 0.038 17.728 0.000 #&gt; kesempatan =~ #&gt; F1 0.350 0.022 16.137 0.000 #&gt; F2 0.352 0.021 16.726 0.000 #&gt; F3 0.361 0.022 16.232 0.000 #&gt; F4 0.330 0.022 14.836 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.539 0.672 #&gt; 0.702 0.854 #&gt; 0.752 0.864 #&gt; 0.720 0.855 #&gt; #&gt; 0.724 0.813 #&gt; 0.739 0.935 #&gt; #&gt; 0.692 0.892 #&gt; 0.717 0.912 #&gt; #&gt; 0.723 0.922 #&gt; 0.703 0.941 #&gt; 0.702 0.855 #&gt; 0.590 0.797 #&gt; #&gt; 0.736 0.888 #&gt; 0.684 0.897 #&gt; 0.699 0.870 #&gt; 0.747 0.927 #&gt; 0.726 0.894 #&gt; 0.673 0.834 #&gt; #&gt; 0.771 0.900 #&gt; 0.776 0.933 #&gt; 0.796 0.905 #&gt; 0.727 0.833 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; kesempatan ~ #&gt; faktor 0.031 0.186 0.167 0.867 #&gt; permintaan 0.087 0.122 0.709 0.478 #&gt; industri 0.253 0.267 0.947 0.344 #&gt; strategi 0.272 0.189 1.442 0.149 #&gt; regulasi 1.441 0.190 7.578 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.014 0.014 #&gt; 0.039 0.039 #&gt; 0.115 0.115 #&gt; 0.123 0.123 #&gt; 0.654 0.654 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .A1 ~~ #&gt; .A2 0.068 0.019 3.588 0.000 #&gt; faktor ~~ #&gt; permintaan 0.573 0.046 12.417 0.000 #&gt; industri 0.837 0.025 33.458 0.000 #&gt; strategi 0.716 0.033 21.421 0.000 #&gt; regulasi 0.824 0.024 34.919 0.000 #&gt; permintaan ~~ #&gt; industri 0.729 0.035 20.581 0.000 #&gt; strategi 0.746 0.032 23.189 0.000 #&gt; regulasi 0.623 0.041 15.292 0.000 #&gt; industri ~~ #&gt; strategi 0.874 0.020 44.757 0.000 #&gt; regulasi 0.816 0.024 33.429 0.000 #&gt; strategi ~~ #&gt; regulasi 0.762 0.027 27.982 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.068 0.269 #&gt; #&gt; 0.573 0.573 #&gt; 0.837 0.837 #&gt; 0.716 0.716 #&gt; 0.824 0.824 #&gt; #&gt; 0.729 0.729 #&gt; 0.746 0.746 #&gt; 0.623 0.623 #&gt; #&gt; 0.874 0.874 #&gt; 0.816 0.816 #&gt; #&gt; 0.762 0.762 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .A1 0.353 0.032 11.133 0.000 #&gt; .A2 0.182 0.020 9.132 0.000 #&gt; .A3 0.192 0.022 8.905 0.000 #&gt; .A4 0.190 0.021 9.171 0.000 #&gt; .B1 0.270 0.032 8.454 0.000 #&gt; .B2 0.078 0.025 3.155 0.002 #&gt; .C1 0.123 0.014 8.573 0.000 #&gt; .C2 0.104 0.014 7.494 0.000 #&gt; .D1 0.093 0.011 8.748 0.000 #&gt; .D2 0.063 0.008 7.481 0.000 #&gt; .D3 0.182 0.017 10.624 0.000 #&gt; .D4 0.200 0.018 11.218 0.000 #&gt; .E1 0.145 0.014 10.565 0.000 #&gt; .E2 0.114 0.011 10.392 0.000 #&gt; .E3 0.157 0.014 10.844 0.000 #&gt; .E4 0.092 0.010 9.490 0.000 #&gt; .E5 0.132 0.013 10.456 0.000 #&gt; .E6 0.197 0.018 11.222 0.000 #&gt; .F1 0.140 0.014 9.700 0.000 #&gt; .F2 0.090 0.011 8.219 0.000 #&gt; .F3 0.140 0.015 9.538 0.000 #&gt; .F4 0.233 0.021 10.912 0.000 #&gt; faktor 1.000 #&gt; permintaan 1.000 #&gt; industri 1.000 #&gt; strategi 1.000 #&gt; regulasi 1.000 #&gt; .kesempatan 1.000 #&gt; Std.lv Std.all #&gt; 0.353 0.549 #&gt; 0.182 0.270 #&gt; 0.192 0.253 #&gt; 0.190 0.268 #&gt; 0.270 0.339 #&gt; 0.078 0.125 #&gt; 0.123 0.205 #&gt; 0.104 0.169 #&gt; 0.093 0.151 #&gt; 0.063 0.114 #&gt; 0.182 0.270 #&gt; 0.200 0.365 #&gt; 0.145 0.211 #&gt; 0.114 0.195 #&gt; 0.157 0.242 #&gt; 0.092 0.141 #&gt; 0.132 0.201 #&gt; 0.197 0.304 #&gt; 0.140 0.190 #&gt; 0.090 0.130 #&gt; 0.140 0.181 #&gt; 0.233 0.306 #&gt; 1.000 1.000 #&gt; 1.000 1.000 #&gt; 1.000 1.000 #&gt; 1.000 1.000 #&gt; 1.000 1.000 #&gt; 0.206 0.206 7.2.1 Visualisasi SEM semPaths(sem.fit) semPaths(sem.fit, &quot;std&quot;, color = list(lat = &quot;green&quot;, man = &quot;yellow&quot;), edge.color=&quot;black&quot;) semPaths(sem.fit, &quot;std&quot;, color = list(lat = &quot;green&quot;, man = &quot;yellow&quot;), edge.color=&quot;black&quot;, fade=FALSE) semPaths(sem.fit, &quot;std&quot;, color = list(lat = &quot;green&quot;, man = &quot;yellow&quot;), edge.color=&quot;black&quot;, fade=FALSE, residuals=FALSE, exoCov=FALSE) 7.3 PLS SEM # source:https://rpubs.com/ifn1411/PLS # install plspm #install.packages(&quot;plspm&quot;) # load plspm library(plspm) #&gt; Warning: package &#39;plspm&#39; was built under R version 4.4.3 #&gt; #&gt; Attaching package: &#39;plspm&#39; #&gt; The following objects are masked from &#39;package:psych&#39;: #&gt; #&gt; alpha, rescale, unidim # load data spainmodel data(spainfoot) # first 5 row of spainmodel data head(spainfoot) #&gt; GSH GSA SSH SSA GCH GCA CSH CSA WMH WMA LWR #&gt; Barcelona 61 44 0.95 0.95 14 21 0.47 0.32 14 13 10 #&gt; RealMadrid 49 34 1.00 0.84 29 23 0.37 0.37 14 11 10 #&gt; Sevilla 28 26 0.74 0.74 20 19 0.42 0.53 11 10 4 #&gt; AtleMadrid 47 33 0.95 0.84 23 34 0.37 0.16 13 7 6 #&gt; Villarreal 33 28 0.84 0.68 25 29 0.26 0.16 12 6 5 #&gt; Valencia 47 21 1.00 0.68 26 28 0.26 0.26 12 6 5 #&gt; LRWL YC RC #&gt; Barcelona 22 76 6 #&gt; RealMadrid 18 115 9 #&gt; Sevilla 7 100 8 #&gt; AtleMadrid 9 116 5 #&gt; Villarreal 11 102 5 #&gt; Valencia 8 120 6 Attack &lt;- c(0, 0, 0) Defense &lt;- c(1, 0, 0) Success &lt;- c(1, 0, 0) model_path &lt;- rbind(Attack, Defense, Success) colnames(model_path) &lt;- rownames(model_path) model_path #&gt; Attack Defense Success #&gt; Attack 0 0 0 #&gt; Defense 1 0 0 #&gt; Success 1 0 0 # graph structural model innerplot(model_path) Attack &lt;- c(0, 1, 0) Defense &lt;- c(0, 0, 0) Success &lt;- c(1, 1, 0) model_path2 &lt;- rbind(Attack, Defense, Success) colnames(model_path2) &lt;- rownames(model_path2) model_path2 #&gt; Attack Defense Success #&gt; Attack 0 1 0 #&gt; Defense 0 0 0 #&gt; Success 1 1 0 # graph structural model innerplot(model_path2, txt.col = &quot;black&quot;) # define latent variable associated with model_blocks &lt;- list(1:4, 5:8, 9:12) # vector of modes (reflective) model_modes &lt;- c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;) # run plspm analysis model_pls &lt;- plspm(Data = spainfoot, path_matrix = model_path, blocks = model_blocks, modes = model_modes) model_pls #&gt; Partial Least Squares Path Modeling (PLS-PM) #&gt; --------------------------------------------- #&gt; NAME DESCRIPTION #&gt; 1 $outer_model outer model #&gt; 2 $inner_model inner model #&gt; 3 $path_coefs path coefficients matrix #&gt; 4 $scores latent variable scores #&gt; 5 $crossloadings cross-loadings #&gt; 6 $inner_summary summary inner model #&gt; 7 $effects total effects #&gt; 8 $unidim unidimensionality #&gt; 9 $gof goodness-of-fit #&gt; 10 $boot bootstrap results #&gt; 11 $data data matrix #&gt; --------------------------------------------- #&gt; You can also use the function &#39;summary&#39; # Unidimensionality model_pls$unidim #&gt; Mode MVs C.alpha DG.rho eig.1st eig.2nd #&gt; Attack A 4 0.8905919 0.92456079 3.017160 0.7923055 #&gt; Defense A 4 0.0000000 0.02601677 2.393442 1.1752781 #&gt; Success A 4 0.9165491 0.94232868 3.217294 0.5370492 plot(model_pls, what = &quot;loadings&quot;) # Loadings and Communilaties model_pls$outer_model #&gt; name block weight loading communality #&gt; 1 GSH Attack 0.3474771 0.9412506 0.8859527 #&gt; 2 GSA Attack 0.2671782 0.8562398 0.7331465 #&gt; 3 SSH Attack 0.2922077 0.8466039 0.7167381 #&gt; 4 SSA Attack 0.2396012 0.8212987 0.6745316 #&gt; 5 GCH Defense -0.1198790 0.4762965 0.2268583 #&gt; 6 GCA Defense -0.4264164 0.8885714 0.7895590 #&gt; 7 CSH Defense 0.2949470 -0.7297095 0.5324759 #&gt; 8 CSA Defense 0.3898039 -0.8947452 0.8005689 #&gt; 9 WMH Success 0.2484276 0.7884562 0.6216632 #&gt; 10 WMA Success 0.2691511 0.8747163 0.7651285 #&gt; 11 LWR Success 0.2947322 0.9703409 0.9415614 #&gt; 12 LRWL Success 0.2998524 0.9428112 0.8888929 #&gt; redundancy #&gt; 1 0.00000000 #&gt; 2 0.00000000 #&gt; 3 0.00000000 #&gt; 4 0.00000000 #&gt; 5 0.05071506 #&gt; 6 0.17650898 #&gt; 7 0.11903706 #&gt; 8 0.17897028 #&gt; 9 0.49452090 #&gt; 10 0.60864477 #&gt; 11 0.74899365 #&gt; 12 0.70709694 # Crossloadings model_pls$crossloadings #&gt; name block Attack Defense Success #&gt; 1 GSH Attack 0.9412506 -0.5139001 0.9019257 #&gt; 2 GSA Attack 0.8562398 -0.3403294 0.7483558 #&gt; 3 SSH Attack 0.8466039 -0.4124617 0.7781795 #&gt; 4 SSA Attack 0.8212987 -0.3455460 0.6308989 #&gt; 5 GCH Defense -0.1302683 0.4762965 -0.1620567 #&gt; 6 GCA Defense -0.4633220 0.8885714 -0.5640722 #&gt; 7 CSH Defense 0.3204993 -0.7297095 0.4850456 #&gt; 8 CSA Defense 0.4235465 -0.8947452 0.5811253 #&gt; 9 WMH Success 0.7126127 -0.4120502 0.7884562 #&gt; 10 WMA Success 0.7720228 -0.7147787 0.8747163 #&gt; 11 LWR Success 0.8454164 -0.5345709 0.9703409 #&gt; 12 LRWL Success 0.8600973 -0.5910943 0.9428112 # Coefficient of Determination model_pls$inner_model #&gt; $Defense #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; Intercept 5.504973e-17 0.2076918 2.650549e-16 1.00000000 #&gt; Attack -4.728148e-01 0.2076918 -2.276521e+00 0.03526176 #&gt; #&gt; $Success #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; Intercept 7.783183e-17 0.1065936 7.301735e-16 1.000000e+00 #&gt; Attack 8.918971e-01 0.1065936 8.367266e+00 1.285711e-07 # Redundancy model_pls$inner_summary #&gt; Type R2 Block_Communality #&gt; Attack Exogenous 0.0000000 0.7525922 #&gt; Defense Endogenous 0.2235539 0.5873656 #&gt; Success Endogenous 0.7954804 0.8043115 #&gt; Mean_Redundancy AVE #&gt; Attack 0.0000000 0.7525922 #&gt; Defense 0.1313078 0.5873656 #&gt; Success 0.6398141 0.8043115 # Goodness-of-fit model_pls$gof #&gt; [1] 0.6034738 plot(model_pls, what = &quot;inner&quot;, colpos = &quot;#6890c4BB&quot;, colneg = &quot;#f9675dBB&quot;, txt.col = &quot;black&quot;, arr.tcol=&quot;black&quot;) "],["analytic-hierarchy-process-ahp.html", "Chapter 8 Analytic Hierarchy Process (AHP) 8.1 Prosedur Pengolahan AHP", " Chapter 8 Analytic Hierarchy Process (AHP) 8.1 Prosedur Pengolahan AHP 8.1.1 Data ahpdata &lt;- read.csv(&quot;Data/ahp.csv&quot;) ahpdata #&gt; Responden SAL_QL SAL_IW SAL_LC QL_IW QL_LC IW_LC J1_J2 #&gt; 1 1 -5 -2 -4 2 2 -2 -2 #&gt; 2 2 -7 -3 -3 3 3 -4 -3 #&gt; J1_j3 J2_J3 J1_J2.1 J1_j3.1 J2_J3.1 J1_J2.2 J1_j3.2 #&gt; 1 -4 -2 2 3 3 7 3 #&gt; 2 -7 -1 3 3 3 4 -1 #&gt; J2_J3.2 J1_J2.3 J1_j3.3 J2_J3.3 #&gt; 1 -3 4 7 -2 #&gt; 2 -3 5 2 -4 8.1.2 Analisis 8.1.2.1 Faktor # Mendefinisikan faktor faktor &lt;- c(&quot;SAL&quot;, &quot;QL&quot;, &quot;IW&quot;, &quot;LC&quot;) # Menampilkan data frame faktor_data &lt;- ahpdata[, 2:7] faktor_data #&gt; SAL_QL SAL_IW SAL_LC QL_IW QL_LC IW_LC #&gt; 1 -5 -2 -4 2 2 -2 #&gt; 2 -7 -3 -3 3 3 -4 # install.packages(&quot;ahpsurvey&quot;) library(ahpsurvey) faktor_data_mat &lt;- ahp.mat(df = faktor_data, faktor, negconvert = TRUE) faktor_data_mat #&gt; [[1]] #&gt; SAL QL IW LC #&gt; SAL 1.00 5 2.0 4.0 #&gt; QL 0.20 1 0.5 0.5 #&gt; IW 0.50 2 1.0 2.0 #&gt; LC 0.25 2 0.5 1.0 #&gt; #&gt; [[2]] #&gt; SAL QL IW LC #&gt; SAL 1.0000000 7 3.0000000 3.0000000 #&gt; QL 0.1428571 1 0.3333333 0.3333333 #&gt; IW 0.3333333 3 1.0000000 4.0000000 #&gt; LC 0.3333333 3 0.2500000 1.0000000 # Consistency ri &lt;- ahp.ri(nsims = 10000, dim = 4, seed = 42) ahp.cr(faktor_data_mat, faktor, ri) #&gt; [1] 0.01780548 0.09677931 #Treatement Consistency (Jika Tidak Konsisten) #faktor_data_mat &lt;- ahp.harker(faktor_data_mat, faktor, iterations = 10, stopcr = 0.1) #ahp.cr(faktor_data_mat, faktor) The ahp.cr function calculates the consistency ratio of each decision-maker, defined by the following equation: CR = (λ − n)/((n − 1)(RI)) Where λ is the maximum eigenvalue of the pairwise comparison matrix, n is the number of attributes, and RI is the random index. Following Saaty and Tran (2007), the RI is a function of n and is the consistency ratio of randomly generated pairwise comparison matrices. Saaty showed that when the CR is higher than 0.1, the choice is deemed to be inconsistent 8.1.2.2 Individual Rangking Faktor library(tidyverse) #&gt; Warning: package &#39;ggplot2&#39; was built under R version 4.4.3 #&gt; ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ── #&gt; ✔ dplyr 1.1.4 ✔ readr 2.1.5 #&gt; ✔ forcats 1.0.0 ✔ stringr 1.5.1 #&gt; ✔ ggplot2 3.5.2 ✔ tibble 3.2.1 #&gt; ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 #&gt; ✔ purrr 1.0.2 #&gt; ── Conflicts ────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() #&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(tibble) faktor_ind &lt;- ahp.indpref(faktor_data_mat, faktor, method = &quot;arithmetic&quot;) round(faktor_ind, 3) %&gt;% rownames_to_column(&#39;ID&#39;) #&gt; ID SAL QL IW LC #&gt; 1 1 0.512 0.099 0.243 0.147 #&gt; 2 2 0.517 0.066 0.274 0.143 8.1.2.3 Aggregate Rangking Faktor faktor_agg &lt;- ahp.aggpref(faktor_data_mat, faktor, method = &quot;arithmetic&quot;, aggmethod = &quot;arithmetic&quot;) round(faktor_agg, 3) %&gt;% t() #&gt; SAL QL IW LC #&gt; [1,] 0.514 0.082 0.259 0.145 barplot(faktor_agg,main=&quot;Rangking Faktor&quot;) library(ggplot2) # Mengubah Cat menjadi factor dengan label yang diinginkan data = data.frame(&quot;Cat&quot;=row.names(data.frame(faktor_agg)), data.frame(faktor_agg)) data$Cat &lt;- factor(data$Cat, levels = c(&quot;SAL&quot;, &quot;QL&quot;, &quot;IW&quot;, &quot;LC&quot;), labels = c(&quot;Salary&quot;, &quot;Quality of Life&quot;, &quot;Interes in Work&quot;, &quot;Location&quot;)) # Mengurutkan data$warna &lt;- ifelse(data$faktor_agg == max(data$faktor_agg), &quot;terbesar&quot;, &quot;lainnya&quot;) # Buat grafik batang ggplot(data, aes(x = Cat, y = faktor_agg, fill = warna)) + geom_bar(stat = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;terbesar&quot; = &quot;#4682B4&quot;, &quot;lainnya&quot; = &quot;#A9A9A9&quot;)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + # Sembunyikan legenda labs( title = &quot;AHP: Rangking Faktor&quot;, y = &quot;Skor&quot;, x = &quot;&quot;) 8.1.2.4 Alternatif 8.1.2.5 Alternatif untuk Faktor Salary library(dplyr) alternatif &lt;- c(&quot;J1&quot;, &quot;J2&quot;, &quot;J3&quot;) # Menampilkan data frame alternatif_data1 &lt;- ahpdata[,8:10] alternatif1 &lt;- ahp.mat(df = alternatif_data1, atts = alternatif, negconvert = TRUE) alternatif1_agg &lt;- ahp.aggpref(alternatif1, alternatif, method = &quot;arithmetic&quot;, aggmethod = &quot;arithmetic&quot;) round(alternatif1_agg, 3) %&gt;% t() #&gt; J1 J2 J3 #&gt; [1,] 0.628 0.232 0.139 #Consistency ri &lt;- ahp.ri(nsims = 10000, dim = 3, seed = 42) ahp.cr(alternatif1, alternatif, ri) #&gt; [1] 0.00000000 0.07669698 8.1.2.6 Alternatif untuk Faktor Quality of Life alternatif_data2 &lt;- ahpdata[,11:13] alternatif2 &lt;- ahp.mat(df = alternatif_data2, atts = alternatif, negconvert = TRUE) alternatif2_agg &lt;- ahp.aggpref(alternatif2, alternatif, method = &quot;arithmetic&quot;, aggmethod = &quot;arithmetic&quot;) round(alternatif2_agg, 3) %&gt;% t() #&gt; J1 J2 J3 #&gt; [1,] 0.15 0.269 0.581 #Consistency ahp.cr(alternatif2, alternatif, ri) #&gt; [1] 0.05121571 0.12952632 8.1.2.7 Alternatif untuk Faktor Interest in Work alternatif_data3 &lt;- ahpdata[,14:16] alternatif3 &lt;- ahp.mat(df = alternatif_data3, atts = alternatif, negconvert = TRUE) alternatif3_agg &lt;- ahp.aggpref(alternatif3, alternatif, method = &quot;arithmetic&quot;, aggmethod = &quot;arithmetic&quot;) round(alternatif3_agg, 3) %&gt;% t() #&gt; J1 J2 J3 #&gt; [1,] 0.132 0.651 0.218 #Consistency ahp.cr(alternatif3, alternatif, ri) #&gt; [1] 0.006706716 0.008789809 8.1.2.8 Alternatif untuk Faktor Location alternatif_data4 &lt;- ahpdata[,17:19] alternatif4 &lt;- ahp.mat(df = alternatif_data4, atts = alternatif, negconvert = TRUE) alternatif4_agg &lt;- ahp.aggpref(alternatif4, alternatif, method = &quot;arithmetic&quot;, aggmethod = &quot;arithmetic&quot;) round(alternatif4_agg, 3) %&gt;% t() #&gt; J1 J2 J3 #&gt; [1,] 0.104 0.597 0.299 #Consistency ahp.cr(alternatif4, alternatif, ri) #&gt; [1] 0.16898990 0.02349155 8.1.2.9 Gabungan Alternatif alternatif_agg &lt;- cbind(alternatif1_agg,alternatif2_agg, alternatif3_agg,alternatif4_agg) %*% faktor_agg alternatif_agg #&gt; [,1] #&gt; J1 0.3844544 #&gt; J2 0.3964920 #&gt; J3 0.2190537 barplot(t(alternatif_agg) ,main=&quot;Rangking Alternatif&quot;) data = data.frame(&quot;Cat&quot;=row.names(data.frame(alternatif_agg)), data.frame(alternatif_agg)) data$Cat &lt;- factor(data$Cat, levels = c( &quot;J1&quot; , &quot;J2&quot; ,&quot;J3&quot;), labels = c(&quot;Job1&quot;, &quot;Job2&quot;,&quot;Job3&quot;)) # Buat grafik batang data$warna &lt;- ifelse(data$alternatif_agg == max(data$alternatif_agg), &quot;terbesar&quot;, &quot;lainnya&quot;) # Buat grafik batang ggplot(data, aes(x = Cat, y = alternatif_agg, fill = warna)) + geom_bar(stat = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;terbesar&quot; = &quot;#4682B4&quot;, &quot;lainnya&quot; = &quot;#A9A9A9&quot;)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs( title = &quot;AHP: Rangking Alternatif&quot;, y = &quot;Skor&quot;, x = &quot;&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
